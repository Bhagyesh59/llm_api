{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Using cached groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.49.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: python-dotenv in ./env/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./env/lib/python3.11/site-packages (from groq) (4.6.0)\n",
      "Collecting distro<2,>=1.7.0 (from groq)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from groq)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./env/lib/python3.11/site-packages (from groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in ./env/lib/python3.11/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./env/lib/python3.11/site-packages (from groq) (4.12.2)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: idna>=2.8 in ./env/lib/python3.11/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->groq)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./env/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./env/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
      "Using cached groq-0.11.0-py3-none-any.whl (106 kB)\n",
      "Downloading openai-1.49.0-py3-none-any.whl (378 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached jiter-0.5.0-cp311-cp311-macosx_11_0_arm64.whl (299 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Installing collected packages: tqdm, jiter, distro, certifi, httpcore, httpx, openai, groq\n",
      "Successfully installed certifi-2024.8.30 distro-1.9.0 groq-0.11.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.49.0 tqdm-4.66.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install groq openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load GROQ_API_KEY from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some potential product names based on the description and seed words:\n",
      "\n",
      "1. QuickShake - combines the idea of fast with a playful nod to the product's purpose\n",
      "2. NutriBlend - emphasizes the healthy aspect and the blending function\n",
      "3. RapidMix - highlights the speed and mixing capability of the product\n",
      "4. CompactCreamer - incorporates the compact design and a hint at the creamy treats it can produce\n",
      "5. HealthyHurricane - suggests a fast and powerful mixing action while emphasizing the healthy benefits\n",
      "6. SwiftSip - evokes the idea of quickly whipping up a refreshing drink\n",
      "7. BlendBurst - conveys the idea of fast and powerful blending\n",
      "8. FitFroth - combines the health focus with a nod to the frothy texture of milkshakes\n",
      "9. ZoomZest - suggests a fun, fast, and lively experience\n",
      "10. PurePulse - emphasizes the healthy aspect and a sense of energetic mixing action.\n",
      "\n",
      "Let me know if you'd like me to generate more options!\n",
      "CPU times: user 35 ms, sys: 15.4 ms, total: 50.4 ms\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from openai import OpenAI\n",
    "\n",
    "system_content = \"You will be provided with a product description and seed words. Your task is to generate potential product names.\"\n",
    "user_content = \"Product description: A home milkshake maker. Seed words: fast, healthy, compact.\"\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key = API_KEY,\n",
    "  base_url = BASE_URL\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"llama3-70b-8192\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_content},\n",
    "    {\"role\": \"user\", \"content\": user_content}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As the sun set over the ocean, a lone seagull soared overhead. Below, a young girl named Lily sat on the beach, her toes curled over the edge of the waves. She had always felt drawn to the sea, and today she had finally found the courage to explore its secrets. As she wandered along the shoreline, she stumbled upon a hidden cave, its entrance guarded by a ancient stone statue. With a thrill of excitement, Lily stepped inside, and discovered a treasure trove of glittering shells and shimmering pearls, hidden away for centuries.None"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from openai import OpenAI\n",
    "\n",
    "system_prompt = \"\"\n",
    "user_prompt = \"write a story in 100 words\"\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key = API_KEY,\n",
    "  base_url = BASE_URL\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"llama3-70b-8192\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ],\n",
    "  \n",
    "  temperature=0,\n",
    "  stream=True,  # this time, we set stream=True\n",
    ")\n",
    "# print(response)\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end = \"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Job Title: Human Resources Assistant Job Description: This position reports to the Human Resources (HR) director and interfaces with company managers and HR staff. Company XYZ is committed to an employee-orientated, high performance culture that emphasizes empowerment, quality, continuous improvement, and the recruitment and ongoing development of a superior workforce. The intern will gain exposure to these functional areas: HR Information Systems; Employee relations; Training and development; Benefits; Compensation; Organization development; Employment Specific responsibilities: - Employee orientation and training logistics and recordkeeping - Company-wide committee facilitation and participation - Employee safety, welfare, wellness and health reporting - Provide direct support to employees during implementation of HR services, policies and programs What skills will the intern learn: - Active participation in strategic planning process, including developing goals, objectives and processes - How to engage professionally in HR meetings and seminars with other HR professionals in the region - Gain experience with Human Resources Information system (HRIS) database management and record keeping - Application of HR law and compliance with governmental regulations Qualifications: - Proficient with Microsoft Word and Excel - General knowledge of employment law and practices - Able to maintain a high level of confidentiality - Effective oral and written management communication skills\n"
     ]
    }
   ],
   "source": [
    "def remove_newlines(text):\n",
    "    # Replace newlines and extra spaces with a single space\n",
    "    clean_text = ' '.join(text.splitlines())\n",
    "    return clean_text\n",
    "\n",
    "# Input text\n",
    "job_description = \"\"\"\n",
    "Job Title: Human Resources Assistant\n",
    "Job Description: This position reports to the Human Resources (HR) director and\n",
    "interfaces with company managers and HR staff. Company XYZ is\n",
    "committed to an employee-orientated, high performance culture that\n",
    "emphasizes empowerment, quality, continuous improvement, and the\n",
    "recruitment and ongoing development of a superior workforce.\n",
    "The intern will gain exposure\n",
    "to these functional areas: HR Information Systems; Employee relations; Training and development;\n",
    "Benefits; Compensation; Organization development; Employment\n",
    "Specific responsibilities: - Employee orientation and training logistics and recordkeeping\n",
    "- Company-wide committee facilitation and participation\n",
    "- Employee safety, welfare, wellness and health reporting\n",
    "- Provide direct support to employees during implementation of HR\n",
    "services, policies and programs\n",
    "What skills will the\n",
    "intern learn: - Active participation in strategic planning process, including\n",
    "developing goals, objectives and processes\n",
    "- How to engage professionally in HR meetings and seminars with\n",
    "other HR professionals in the region\n",
    "- Gain experience with Human Resources Information system (HRIS)\n",
    "database management and record keeping\n",
    "- Application of HR law and compliance with governmental regulations\n",
    "Qualifications: - Proficient with Microsoft Word and Excel\n",
    "- General knowledge of employment law and practices\n",
    "- Able to maintain a high level of confidentiality\n",
    "- Effective oral and written management communication skills\n",
    "\"\"\"\n",
    "\n",
    "# Call the function\n",
    "cleaned_description = remove_newlines(job_description)\n",
    "\n",
    "# Output the cleaned text\n",
    "print(cleaned_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      \"Here is the structured summary in YAML format:\n",
      "\n",
      "```\n",
      "Title: Human Resources Assistant\n",
      "Keywords: Microsoft Word, Excel, HRIS\n",
      "Education: Not specified\n",
      "Location: Not specified\n",
      "Summary: |\n",
      " The Human Resources Assistant will support the HR director and staff in various functional areas, including HR Information Systems, employee relations, training, benefits, compensation, and organization development. The intern will gain experience in employee orientation, training logistics, committee facilitation, and HR service implementation, while developing skills in strategic planning, HR law, and database management.\n",
      "```\"\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "      \"Here is the structured summary in YAML format:\\n\\n```\\nTitle: Human Resources Assistant\\nKeywords: Microsoft Word, Excel, HRIS\\nEducation: Not specified\\nLocation: Not specified\\nSummary: |\\n The Human Resources Assistant will support the HR director and staff in various functional areas, including HR Information Systems, employee relations, training, benefits, compensation, and organization development. The intern will gain experience in employee orientation, training logistics, committee facilitation, and HR service implementation, while developing skills in strategic planning, HR law, and database management.\\n```\"\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyyaml\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "Installing collected packages: pyyaml\n",
      "Successfully installed pyyaml-6.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyyaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': 'Human Resources Assistant', 'Keywords': 'Microsoft Word, Excel, HRIS', 'Education': 'Not specified', 'Location': 'Not specified', 'Summary': 'Assist the HR director and staff in various HR functions, including employee orientation, training, and development, benefits, compensation, and organization development. Provide support to employees during implementation of HR services, policies, and programs. Gain experience in HRIS database management, record keeping, and application of HR law and compliance with governmental regulations.'}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import re\n",
    "\n",
    "def extract_and_parse_yaml(output_text):\n",
    "    \"\"\"\n",
    "    Extract the YAML content from a string with additional text and parse it into a dictionary.\n",
    "\n",
    "    :param output_text: The text containing the YAML output enclosed within ```\n",
    "    :return: A dictionary with the parsed YAML content\n",
    "    \"\"\"\n",
    "    # Use a regular expression to extract the YAML content between ``` markers\n",
    "    yaml_content = re.search(r\"```(.*?)```\", output_text, re.DOTALL)\n",
    "    \n",
    "    if yaml_content:\n",
    "        yaml_data = yaml_content.group(1).strip()  # Extract the YAML and remove extra whitespace\n",
    "        try:\n",
    "            parsed_data = yaml.safe_load(yaml_data)\n",
    "            return parsed_data\n",
    "        except yaml.YAMLError as e:\n",
    "            print(f\"Error parsing YAML: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No YAML content found.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "output_text = \"\"\"\n",
    "\"Here is the structured summary in YAML format:\\n\\n```\\nTitle: Human Resources Assistant\\nKeywords: Microsoft Word, Excel, HRIS\\nEducation: Not specified\\nLocation: Not specified\\nSummary: |\\n Assist the HR director and staff in various HR functions, including employee orientation, training, and development, benefits, compensation, and organization development. Provide support to employees during implementation of HR services, policies, and programs. Gain experience in HRIS database management, record keeping, and application of HR law and compliance with governmental regulations.\\n```\"\n",
    "\"\"\"\n",
    "parsed_data = extract_and_parse_yaml(output_text)\n",
    "\n",
    "if parsed_data:\n",
    "    print(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Human Resources Assistant\n",
      "Keywords: Microsoft Word, Excel, HRIS\n",
      "Education: Not specified\n",
      "Location: Not specified\n",
      "Summary: |\n",
      "Assist the HR director and staff in various HR functions, including employee orientation, training, and development, benefits, compensation, and organization development. Provide support to employees during implementation of HR services, policies, and programs. Gain experience in HRIS database management, record keeping, and application of HR law and compliance with governmental regulations.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_yaml(output_text):\n",
    "    \"\"\"\n",
    "    Extract and return the YAML content from a string with additional text.\n",
    "\n",
    "    :param output_text: The text containing the YAML output enclosed within ```\n",
    "    :return: The YAML content as a string\n",
    "    \"\"\"\n",
    "    # Use a regular expression to extract the YAML content between ``` markers\n",
    "    yaml_content = re.search(r\"```(.*?)```\", output_text, re.DOTALL)\n",
    "    \n",
    "    if yaml_content:\n",
    "        return yaml_content.group(1).strip()  # Extract the YAML and remove extra whitespace\n",
    "    else:\n",
    "        print(\"No YAML content found.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "output_text = \"\"\"\n",
    "Here is the structured summary in YAML format:\n",
    "```\n",
    "Title: Human Resources Assistant\n",
    "Keywords: Microsoft Word, Excel, HRIS\n",
    "Education: Not specified\n",
    "Location: Not specified\n",
    "Summary: |\n",
    "Assist the HR director and staff in various HR functions, including employee orientation, training, and development, benefits, compensation, and organization development. Provide support to employees during implementation of HR services, policies, and programs. Gain experience in HRIS database management, record keeping, and application of HR law and compliance with governmental regulations.\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "yaml_output = extract_yaml(output_text)\n",
    "\n",
    "if yaml_output:\n",
    "    print(yaml_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./env/lib/python3.11/site-packages (0.3.1)\n",
      "Collecting langchain_groq\n",
      "  Downloading langchain_groq-0.2.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./env/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./env/lib/python3.11/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./env/lib/python3.11/site-packages (from langchain) (3.10.7)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in ./env/lib/python3.11/site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in ./env/lib/python3.11/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./env/lib/python3.11/site-packages (from langchain) (0.1.129)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./env/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./env/lib/python3.11/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in ./env/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./env/lib/python3.11/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in ./env/lib/python3.11/site-packages (from langchain_groq) (0.11.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./env/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain_groq) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./env/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./env/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.2)\n",
      "Requirement already satisfied: sniffio in ./env/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./env/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./env/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./env/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./env/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./env/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain) (3.0.0)\n",
      "Downloading langchain_groq-0.2.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: langchain_groq\n",
      "Successfully installed langchain_groq-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain_groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pn/k9g3my_96pn3mhxkypnjv00c0000gn/T/ipykernel_1522/4154095683.py:48: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=\"chat_history\", return_messages=True)\n",
      "/var/folders/pn/k9g3my_96pn3mhxkypnjv00c0000gn/T/ipykernel_1522/4154095683.py:94: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  conversation = LLMChain(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\ntools\n  Extra inputs are not permitted [type=extra_forbidden, input_value=[{'type': 'function', 'fu...equired': ['option']}}}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\ntool_choice\n  Extra inputs are not permitted [type=extra_forbidden, input_value='auto', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 94\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m     78\u001b[0m     [\n\u001b[1;32m     79\u001b[0m         SystemMessage(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m     ]\n\u001b[1;32m     91\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Create a conversation chain using the LangChain LLM (Language Learning Model)\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m conversation \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroq_chat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The Groq LangChain chat object initialized earlier.\u001b[39;49;00m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The constructed prompt template.\u001b[39;49;00m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# TRUE Enables verbose output, which can be useful for debugging.\u001b[39;49;00m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The conversational memory object that stores and manages the conversation history.\u001b[39;49;00m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# The chatbot's answer is generated by sending the full prompt to the Groq API.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m response \u001b[38;5;241m=\u001b[39m conversation\u001b[38;5;241m.\u001b[39mpredict(human_input\u001b[38;5;241m=\u001b[39muser_question)\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     emit_warning()\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/langchain_core/load/serializable.py:110\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/pydantic/main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for LLMChain\ntools\n  Extra inputs are not permitted [type=extra_forbidden, input_value=[{'type': 'function', 'fu...equired': ['option']}}}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\ntool_choice\n  Extra inputs are not permitted [type=extra_forbidden, input_value='auto', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    This function is the main entry point of the application. It sets up the Groq client, the Streamlit interface, and handles the chat interaction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get Groq API key\n",
    "    groq_api_key = os.environ['GROQ_API_KEY']\n",
    "    model = 'llama3-8b-8192'\n",
    "    # Initialize Groq Langchain chat object and conversation\n",
    "    groq_chat = ChatGroq(\n",
    "            groq_api_key=groq_api_key, \n",
    "            model_name=model\n",
    "    )\n",
    "    \n",
    "    job_description = \"\"\"\n",
    "    Senior Python Developer: 5+ years of experience in Python, Django, and Flask. Familiar with cloud platforms like AWS and Azure. \n",
    "    Must have strong communication skills and experience working in agile teams. Remote work available.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt=f'''\n",
    "    You are a job assistant who is tasked with answering student questions with the help of job description\n",
    "    if you do not know or the answer is not present in the job description just say it is not specified.\n",
    "    do not make up answers.\n",
    "    if student asked about salary related question only then call the call_recruiter function\n",
    "    For successful task completion:  \n",
    "    Thought: Consider the task at hand and determine which tool is best suited\n",
    "    Thought: Follow a following set questions to ask the student\n",
    "        1: \n",
    "    based on its capabilities and the nature of the work. \n",
    "    Help answer questions about the following job description: \n",
    "    {job_description}\n",
    "    '''\n",
    "    conversational_memory_length = 5 # number of previous messages the chatbot will remember during the conversation\n",
    "\n",
    "    memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=\"chat_history\", return_messages=True)\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"call_recruiter\",\n",
    "                \"description\": \"Call a recruiter to assist with queries related to salary only.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"option\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"An integer value, '1' to connect with a recruiter\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"option\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    #chat_history = []\n",
    "    while True:\n",
    "        user_question = input(\"Ask a question: \")\n",
    "\n",
    "        # If the user has asked a question,\n",
    "        if user_question:\n",
    "\n",
    "            # Construct a chat prompt template using various components\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=system_prompt\n",
    "                    ),  # This is the persistent system prompt that is always included at the start of the chat.\n",
    "\n",
    "                    MessagesPlaceholder(\n",
    "                        variable_name=\"chat_history\"\n",
    "                    ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.\n",
    "\n",
    "                    HumanMessagePromptTemplate.from_template(\n",
    "                        \"{human_input}\"\n",
    "                    ),  # This template is where the user's current input will be injected into the prompt.\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Create a conversation chain using the LangChain LLM (Language Learning Model)\n",
    "            conversation = LLMChain(\n",
    "                llm=groq_chat,  # The Groq LangChain chat object initialized earlier.\n",
    "                prompt=prompt,  # The constructed prompt template.\n",
    "                verbose=False,   # TRUE Enables verbose output, which can be useful for debugging.\n",
    "                memory=memory,\n",
    "                tools=tools,    # The conversational memory object that stores and manages the conversation history.\n",
    "                tool_choice=\"auto\"\n",
    "            )\n",
    "            # The chatbot's answer is generated by sending the full prompt to the Groq API.\n",
    "            response = conversation.predict(human_input=user_question)\n",
    "            print(\"Chatbot:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pn/k9g3my_96pn3mhxkypnjv00c0000gn/T/ipykernel_79701/807592394.py:23: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "groq_api_key = os.environ['GROQ_API_KEY']\n",
    "model = 'llama3-8b-8192'\n",
    "    # Initialize Groq Langchain chat object and conversation\n",
    "groq_chat = ChatGroq(\n",
    "            groq_api_key=groq_api_key, \n",
    "            model_name=model\n",
    "    )\n",
    "system_prompt = 'You are a friendly conversational chatbot'\n",
    "conversational_memory_length = 5 # number of previous messages the chatbot will remember during the conversation\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm here! I'm just a computer program, so I don't have a physical location, but I'm always happy to chat with you whenever you need someone to talk to!\n",
      "Chatbot: Glad to hear that! So, what's been going on in your world lately? Any exciting plans or adventures coming up? I'm all ears (or rather, all text) if you want to share!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m         user_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAsk a question: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# If the user has asked a question,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m user_question:\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m             \u001b[38;5;66;03m# Construct a chat prompt template using various components\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "        user_question = input(\"Ask a question: \")\n",
    "\n",
    "        # If the user has asked a question,\n",
    "        if user_question:\n",
    "\n",
    "            # Construct a chat prompt template using various components\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=system_prompt\n",
    "                    ),  # This is the persistent system prompt that is always included at the start of the chat.\n",
    "\n",
    "                    MessagesPlaceholder(\n",
    "                        variable_name=\"chat_history\"\n",
    "                    ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.\n",
    "\n",
    "                    HumanMessagePromptTemplate.from_template(\n",
    "                        \"{human_input}\"\n",
    "                    ),  # This template is where the user's current input will be injected into the prompt.\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Create a conversation chain using the LangChain LLM (Language Learning Model)\n",
    "            conversation = LLMChain(\n",
    "                llm=groq_chat,  # The Groq LangChain chat object initialized earlier.\n",
    "                prompt=prompt,  # The constructed prompt template.\n",
    "                verbose=False,   # TRUE Enables verbose output, which can be useful for debugging.\n",
    "                memory=memory,  # The conversational memory object that stores and manages the conversation history.\n",
    "            )\n",
    "            \n",
    "            response = conversation.predict(human_input=user_question)\n",
    "            print(\"Chatbot:\", response)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: helo man\n",
      "AI: Hey there! How's it going? It's great to chat with you! Is there something on your mind that you'd like to talk about, or are you just looking for some friendly conversation?\n",
      "Human: what is 2+2 \n",
      "AI: That's an easy one! The answer is 4! Would you like to try another math problem or move on to something else?\n",
      "Human: hi ho ware you\n",
      "AI: I'm here! I'm just a computer program, so I don't have a physical location, but I'm always happy to chat with you whenever you need someone to talk to!\n",
      "Human: ok good\n",
      "AI: Glad to hear that! So, what's been going on in your world lately? Any exciting plans or adventures coming up? I'm all ears (or rather, all text) if you want to share!\n"
     ]
    }
   ],
   "source": [
    "print(memory.chat_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''You are tasked with completing specific objectives and \n",
    "must report the outcomes. At your disposal, you have a variety of tools, \n",
    "each specialized in performing a distinct type of task.  \n",
    "  \n",
    "For successful task completion:  \n",
    "Thought: Consider the task at hand and determine which tool is best suited \n",
    "based on its capabilities and the nature of the work.  \n",
    "  \n",
    "Use the report_tool with an instruction detailing the results of your work.  \n",
    "If you encounter an issue and cannot complete the task:  \n",
    "  \n",
    "Use the report_tool to communicate the challenge or reason for the \n",
    "task's incompletion.  \n",
    "You will receive feedback based on the outcomes of \n",
    "each tool's task execution or explanations for any tasks that \n",
    "couldn't be completed. This feedback loop is crucial for addressing \n",
    "and resolving any issues by strategically deploying the available tools.  \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key = API_KEY,\n",
    "  base_url = BASE_URL\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"llama3-8b-8192\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"hello how are\"}\n",
    "  ],\n",
    "  tools=[\n",
    "          {\n",
    "            \"type\": \"function\",\n",
    "              \"function\": {\n",
    "                \"name\": \"call_recruiter\",\n",
    "                    \"description\": \"to call recruiter the required information is missing or you are unsure\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"call\": {\n",
    "                                \"type\": \"integer\",  # Ensure this is 'integer' (valid JSON type)\n",
    "                                \"description\": \"retturn 1 to call the recruiter\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"call\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "  tool_choice = \"auto\",\n",
    "  temperature=0,\n",
    "  # stream=True,  # this time, we set stream=True\n",
    ")\n",
    "# print(response)\n",
    "\n",
    "# for chunk in response:\n",
    "#     print(chunk.choices[0].delta.content, end = \"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_6b35', function=Function(arguments='{\"date\":\"2023-03-16T14:30:00.000Z\",\"description\":\"Coffee\",\"gross_amount\":5,\"net_amount\":4.0,\"tax_rate\":0.2}', name='Expense'), type='function')]))]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function(arguments='{\"date\":\"2023-03-16T14:30:00.000Z\",\"description\":\"Coffee\",\"gross_amount\":5,\"net_amount\":4.0,\"tax_rate\":0.2}', name='Expense')\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.tool_calls[0].function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from groq import Groq\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the Groq client\n",
    "client = OpenAI(\n",
    "  api_key = API_KEY,\n",
    "  base_url = BASE_URL\n",
    ")\n",
    "# Specify the model to be used (we recommend our fine-tuned models or the Llama 3.1 models)\n",
    "MODEL = 'llama3-70b-8192'\n",
    "\n",
    "def calculate(expression):\n",
    "    \"\"\"Evaluate a mathematical expression\"\"\"\n",
    "    try:\n",
    "        # Attempt to evaluate the math expression\n",
    "        result = eval(expression)\n",
    "        print(result)\n",
    "        return json.dumps({\"result\": result})\n",
    "    except:\n",
    "        # Return an error message if the math expression is invalid\n",
    "        return json.dumps({\"error\": \"Invalid expression\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34220\n",
      "The result of the calculation 34215 + 5 is indeed 34220.\n"
     ]
    }
   ],
   "source": [
    "# imports calculate function from step 1\n",
    "# MODEL=\"llama3-8b-8192\"\n",
    "def run_conversation(user_prompt):\n",
    "    # Initialize the conversation with system and user messages\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a calculator assistant. Use the calculate function to perform mathematical operations and provide the results.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        }\n",
    "    ]\n",
    "    # Define the available tools (i.e. functions) for our model to use\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate\",\n",
    "                \"description\": \"Evaluate a mathematical expression\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The mathematical expression to evaluate\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"expression\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    # Make the initial API call to Groq\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL, # LLM to use\n",
    "        messages=messages, # Conversation history\n",
    "        stream=False,\n",
    "        tools=tools, # Available tools (i.e. functions) for our LLM to use\n",
    "        tool_choice=\"auto\", # Let our LLM decide when to use tools\n",
    "        max_tokens=4096 # Maximum number of tokens to allow in our response\n",
    "    )\n",
    "    # Extract the response and any tool call responses\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    if tool_calls:\n",
    "        # Define the available tools that can be called by the LLM\n",
    "        available_functions = {\n",
    "            \"calculate\": calculate,\n",
    "        }\n",
    "        # Add the LLM's response to the conversation\n",
    "        messages.append(response_message)\n",
    "\n",
    "        # Process each tool call\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            # Call the tool and get the response\n",
    "            function_response = function_to_call(\n",
    "                expression=function_args.get(\"expression\")\n",
    "            )\n",
    "            # Add the tool response to the conversation\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id, \n",
    "                    \"role\": \"tool\", # Indicates this message is from tool use\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )\n",
    "        # Make a second API call with the updated conversation\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "        # Return the final response\n",
    "        return second_response.choices[0].message.content\n",
    "    \n",
    "    else:\n",
    "        return  response.choices[0].message.content\n",
    "# Example usage\n",
    "user_prompt = \"what is 34215 +5\"\n",
    "print(run_conversation(user_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the function that simulates calling a recruiter\n",
    "def call_recruiter(option):\n",
    "    if option == 1:\n",
    "        return \"Connecting to the recruiter now...\"\n",
    "    else:\n",
    "        return \"Invalid option. Please try again.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the recruiter now...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "job_description = \"\"\"\n",
    "Senior Python Developer: 5+ years of experience in Python, Django, and Flask. Familiar with cloud platforms like AWS and Azure. \n",
    "Must have strong communication skills and experience working in agile teams. Remote work available.\n",
    "\"\"\"\n",
    "\n",
    "system_p=f'''\n",
    "You are a job assistant who is tasked with answering student questions with the help of job description\n",
    "if you do not know or the answer is not present in the job description just say it is not specified.\n",
    "do not make up answers.\n",
    "if student asked about salary related question only then call the call_recruiter function\n",
    "For successful task completion:  \n",
    "Thought: Consider the task at hand and determine which tool is best suited \n",
    "based on its capabilities and the nature of the work. \n",
    "Help answer questions about the following job description: \n",
    "{job_description}\n",
    "'''\n",
    "user_prompt = \"what is the salary expections for this job\"\n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":system_p\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": user_prompt,\n",
    "        }\n",
    "    ]\n",
    "tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"call_recruiter\",\n",
    "                \"description\": \"Call a recruiter to assist with queries related to salary only.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"option\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"An integer value, '1' to connect with a recruiter\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"option\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "# Make the initial API call to the LLM\n",
    "response = client.chat.completions.create(\n",
    "        model=MODEL, # LLM to use\n",
    "        messages=messages, # Conversation history\n",
    "        stream=False,\n",
    "        tools=tools, # Available tools (i.e. functions) for our LLM to use\n",
    "        tool_choice=\"auto\", # Let our LLM decide when to use tools\n",
    "        max_tokens=4096 # Maximum number of tokens to allow in our response\n",
    "    )\n",
    "\n",
    "# Extract the response and any tool call responses\n",
    "response_message = response.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "if tool_calls:\n",
    "        available_functions = {\n",
    "            \"call_recruiter\": call_recruiter,\n",
    "        }\n",
    "\n",
    "        # Add the LLM's response to the conversation\n",
    "        messages.append(response_message)\n",
    "\n",
    "        # Process each tool call\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name  # Accessing as attributes, not as dictionary keys\n",
    "            function_to_call =  available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            # Call the tool and get the response\n",
    "            function_response = function_to_call(\n",
    "                option=function_args.get(\"option\")\n",
    "            )\n",
    "            \n",
    "            # Add the tool response to the conversation\n",
    "            message = response.choices[0].message\n",
    "            print(function_response)\n",
    "    \n",
    "else:\n",
    "        print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"option\":1}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message.tool_calls[0].function.arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm your friendly Groq chatbot. I can help answer your questions, provide information, or just chat. I'm also super fast! Let's start our conversation!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 38\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#chat_history = []\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     user_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAsk a question: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# If the user has asked a question,\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_question:\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m         \u001b[38;5;66;03m# Construct a chat prompt template using various components\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    This function is the main entry point of the application. It sets up the Groq client, the Streamlit interface, and handles the chat interaction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get Groq API key\n",
    "    groq_api_key = os.environ['GROQ_API_KEY']\n",
    "    model = 'llama3-8b-8192'\n",
    "    # Initialize Groq Langchain chat object and conversation\n",
    "    groq_chat = ChatGroq(\n",
    "            groq_api_key=groq_api_key, \n",
    "            model_name=model\n",
    "    )\n",
    "    \n",
    "    print(\"Hello! I'm your friendly Groq chatbot. I can help answer your questions, provide information, or just chat. I'm also super fast! Let's start our conversation!\")\n",
    "\n",
    "    system_prompt = 'You are a friendly conversational chatbot'\n",
    "    conversational_memory_length = 5 # number of previous messages the chatbot will remember during the conversation\n",
    "\n",
    "    memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "\n",
    "    #chat_history = []\n",
    "    while True:\n",
    "        user_question = input(\"Ask a question: \")\n",
    "\n",
    "        # If the user has asked a question,\n",
    "        if user_question:\n",
    "\n",
    "            # Construct a chat prompt template using various components\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=system_prompt\n",
    "                    ),  # This is the persistent system prompt that is always included at the start of the chat.\n",
    "\n",
    "                    MessagesPlaceholder(\n",
    "                        variable_name=\"chat_history\"\n",
    "                    ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.\n",
    "\n",
    "                    HumanMessagePromptTemplate.from_template(\n",
    "                        \"{human_input}\"\n",
    "                    ),  # This template is where the user's current input will be injected into the prompt.\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Create a conversation chain using the LangChain LLM (Language Learning Model)\n",
    "            conversation = LLMChain(\n",
    "                llm=groq_chat,  # The Groq LangChain chat object initialized earlier.\n",
    "                prompt=prompt,  # The constructed prompt template.\n",
    "                verbose=False,   # TRUE Enables verbose output, which can be useful for debugging.\n",
    "                memory=memory,  # The conversational memory object that stores and manages the conversation history.\n",
    "            )\n",
    "            # The chatbot's answer is generated by sending the full prompt to the Groq API.\n",
    "            response = conversation.predict(human_input=user_question)\n",
    "            print(\"Chatbot:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"'messages.0' : value must be an object with the discriminator property: 'role'\", 'type': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m conversation_state \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     74\u001b[0m message_from_candidate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am looking for a new opportunity.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 75\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_candidate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_from_candidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m, in \u001b[0;36mhandle_candidate_chat\u001b[0;34m(message, conversation_state)\u001b[0m\n\u001b[1;32m     35\u001b[0m current_question \u001b[38;5;241m=\u001b[39m questions[conversation_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_question\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     36\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease ask the candidate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 38\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3-70b-8192\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\n\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Simulate answer extraction logic (in a real-world scenario, you might have a more sophisticated answer extraction)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/openai/resources/chat/completions.py:704\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    702\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    703\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/openai/_base_client.py:1270\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1258\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1265\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1267\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1268\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1269\u001b[0m     )\n\u001b[0;32m-> 1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/openai/_base_client.py:947\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    945\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/openai/_base_client.py:1051\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1048\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1050\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1054\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1055\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1059\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1060\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'messages.0' : value must be an object with the discriminator property: 'role'\", 'type': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "  api_key = API_KEY,\n",
    "  base_url = BASE_URL\n",
    ")\n",
    "# Define the function that handles the conversation\n",
    "def handle_candidate_chat(message: str, conversation_state: dict):\n",
    "    \"\"\"\n",
    "    Function to handle chat with a candidate. It asks sequential questions based on\n",
    "    previous responses and checks if answers are found.\n",
    "\n",
    "    Args:\n",
    "        message (str): The message from the candidate.\n",
    "        conversation_state (dict): Keeps track of the chat state, including which questions have been asked.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains either the next question or the found answer in JSON format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the sequence of questions to ask\n",
    "    questions = [\n",
    "        \"What is your current job title?\",\n",
    "        \"How many years of experience do you have?\",\n",
    "        \"What is your desired salary range?\",\n",
    "        \"Which technologies are you proficient in?\",\n",
    "        \"Are you open to relocating?\",\n",
    "    ]\n",
    "\n",
    "    # If the conversation is just starting\n",
    "    if \"current_question\" not in conversation_state:\n",
    "        conversation_state[\"current_question\"] = 0\n",
    "        conversation_state[\"answers\"] = {}\n",
    "    \n",
    "    # Send the current question to the LLM\n",
    "    current_question = questions[conversation_state[\"current_question\"]]\n",
    "    prompt = f\"{message}\\n\\nPlease ask the candidate: {current_question}\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages={\"role\": \"user\", \"content\":prompt},\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    llm_output = response.choices[0].text.strip()\n",
    "    \n",
    "    # Simulate answer extraction logic (in a real-world scenario, you might have a more sophisticated answer extraction)\n",
    "    if \"answer\" in llm_output.lower():\n",
    "        answer = llm_output  # Extract the answer here\n",
    "        conversation_state[\"answers\"][current_question] = answer\n",
    "\n",
    "        # Prepare the next question or finish the conversation\n",
    "        conversation_state[\"current_question\"] += 1\n",
    "        \n",
    "        if conversation_state[\"current_question\"] < len(questions):\n",
    "            next_question = questions[conversation_state[\"current_question\"]]\n",
    "            return {\n",
    "                \"next_question\": next_question,\n",
    "                \"answers_so_far\": conversation_state[\"answers\"]\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"status\": \"complete\",\n",
    "                \"final_answers\": conversation_state[\"answers\"]\n",
    "            }\n",
    "    else:\n",
    "        # If no answer found, continue the conversation\n",
    "        return {\n",
    "            \"continue_conversation\": True,\n",
    "            \"llm_response\": llm_output\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "conversation_state = {}\n",
    "message_from_candidate = \"I am looking for a new opportunity.\"\n",
    "result = handle_candidate_chat(message_from_candidate, conversation_state)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m conversation_state \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     84\u001b[0m message_from_candidate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am looking for a new opportunity.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 85\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_candidate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_from_candidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[15], line 43\u001b[0m, in \u001b[0;36mhandle_candidate_chat\u001b[0;34m(message, conversation_state)\u001b[0m\n\u001b[1;32m     40\u001b[0m prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRespond with the answer or ask for clarification if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Call LLM (Replace with appropriate model name)\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3-70b-8192\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Replace with the correct model name if different\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a recruiter chatbot.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Corrected way to access the output\u001b[39;00m\n\u001b[1;32m     53\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set the API key and base URL for OpenAI\n",
    "openai.api_key = API_KEY # Replace with your actual API key\n",
    "openai.api_base = BASE_URL  # Replace with your actual base URL\n",
    "\n",
    "# Define the function that handles the conversation\n",
    "def handle_candidate_chat(message: str, conversation_state: dict):\n",
    "    \"\"\"\n",
    "    Function to handle chat with a candidate. It asks sequential questions based on\n",
    "    previous responses and checks if answers are found.\n",
    "\n",
    "    Args:\n",
    "        message (str): The message from the candidate.\n",
    "        conversation_state (dict): Keeps track of the chat state, including which questions have been asked.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains either the next question or the found answer in JSON format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the sequence of questions to ask\n",
    "    questions = [\n",
    "        \"What is your current job title?\",\n",
    "        \"How many years of experience do you have?\",\n",
    "        \"What is your desired salary range?\",\n",
    "        \"Which technologies are you proficient in?\",\n",
    "        \"Are you open to relocating?\",\n",
    "    ]\n",
    "\n",
    "    # If the conversation is just starting\n",
    "    if \"current_question\" not in conversation_state:\n",
    "        conversation_state[\"current_question\"] = 0\n",
    "        conversation_state[\"answers\"] = {}\n",
    "    \n",
    "    # Get the current question\n",
    "    current_question = questions[conversation_state[\"current_question\"]]\n",
    "    \n",
    "    # Prepare prompt to guide LLM\n",
    "    prompt = f\"Candidate said: '{message}'.\\nPlease ask the candidate the following question: {current_question}.\\n\"\n",
    "    prompt += \"Respond with the answer or ask for clarification if needed.\"\n",
    "\n",
    "    # Call LLM (Replace with appropriate model name)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"llama3-70b-8192\",  # Replace with the correct model name if different\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a recruiter chatbot.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    # Corrected way to access the output\n",
    "    llm_output = response.choices[0].message['content'].strip()\n",
    "\n",
    "    # Simulate answer extraction logic (could be improved with regex or a more sophisticated NLP model)\n",
    "    if \"answer\" in llm_output.lower() or llm_output:  # Adjust based on patterns you expect\n",
    "        answer = llm_output  # Extract the answer from the response\n",
    "        conversation_state[\"answers\"][current_question] = answer\n",
    "\n",
    "        # Move to the next question\n",
    "        conversation_state[\"current_question\"] += 1\n",
    "        \n",
    "        if conversation_state[\"current_question\"] < len(questions):\n",
    "            next_question = questions[conversation_state[\"current_question\"]]\n",
    "            return {\n",
    "                \"next_question\": next_question,\n",
    "                \"answers_so_far\": conversation_state[\"answers\"]\n",
    "            }\n",
    "        else:\n",
    "            # If all questions are asked, return the final answers\n",
    "            return {\n",
    "                \"status\": \"complete\",\n",
    "                \"final_answers\": conversation_state[\"answers\"]\n",
    "            }\n",
    "    else:\n",
    "        # If no clear answer found, continue the conversation\n",
    "        return {\n",
    "            \"continue_conversation\": True,\n",
    "            \"llm_response\": llm_output\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "conversation_state = {}\n",
    "message_from_candidate = \"I am looking for a new opportunity.\"\n",
    "result = handle_candidate_chat(message_from_candidate, conversation_state)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app import database_handler\n",
    "id_='983a0d7b-0154-5c2e-8vfb-056d3ea9ea7f'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsertOneResult(ObjectId('670ec8af87f5452d1c5c9abb'), acknowledged=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_handler.collection.insert_one({\"id_\": id_, \"chat_history\": [], \"chat_summary\":\"\",\"question_answered\":[],\"question_id\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=database_handler.collection.find_one({\"id_\": id_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! I'm excited to help you find a job that fits your skills, preferences, and career goals. Let's get started!\n",
      "\n",
      "To better understand what you're looking for, can you please tell me a little about your current job title and what you do?\n"
     ]
    }
   ],
   "source": [
    "from app.agents import summary, chat\n",
    "prompt=\"this is a test prompt forget any other information given\"\n",
    "llm_output=chat.chat_responce(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=\"Hi there! I'm excited to help you find a job that fits your skills, preferences, and career goals. Let's get started!\\n\\nTo better understand what you're looking for, can you please tell me a little about your current job title and what you do?\", refusal=None, role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['question_id']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
