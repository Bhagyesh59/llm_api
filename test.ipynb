{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in ./env/lib/python3.11/site-packages (0.11.0)\n",
      "Requirement already satisfied: openai in ./env/lib/python3.11/site-packages (1.52.0)\n",
      "Requirement already satisfied: python-dotenv in ./env/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./env/lib/python3.11/site-packages (from groq) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./env/lib/python3.11/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./env/lib/python3.11/site-packages (from groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./env/lib/python3.11/site-packages (from groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in ./env/lib/python3.11/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./env/lib/python3.11/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./env/lib/python3.11/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: tqdm>4 in ./env/lib/python3.11/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: idna>=2.8 in ./env/lib/python3.11/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in ./env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./env/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./env/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install groq openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load GROQ_API_KEY from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some potential product names based on the description and seed words:\n",
      "\n",
      "1. QuickShake - combines the idea of fast with a playful nod to the product's purpose\n",
      "2. NutriBlend - emphasizes the healthy aspect and the blending function\n",
      "3. RapidMix - highlights the speed and mixing capability of the product\n",
      "4. CompactCreamer - incorporates the compact design and a hint at the creamy treats it can produce\n",
      "5. HealthyHurricane - suggests a fast and powerful mixing action while emphasizing the healthy benefits\n",
      "6. SwiftSip - evokes the idea of quickly whipping up a refreshing drink\n",
      "7. BlendBurst - conveys the idea of fast and powerful blending\n",
      "8. FitFroth - combines the health focus with a nod to the frothy texture of milkshakes\n",
      "9. ZoomZest - suggests a fun, fast, and lively experience\n",
      "10. PurePulse - emphasizes the healthy aspect and a sense of energetic mixing action.\n",
      "\n",
      "Let me know if you'd like me to generate more options!\n",
      "CPU times: user 35 ms, sys: 15.4 ms, total: 50.4 ms\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from openai import OpenAI\n",
    "\n",
    "system_content = \"You will be provided with a product description and seed words. Your task is to generate potential product names.\"\n",
    "user_content = \"Product description: A home milkshake maker. Seed words: fast, healthy, compact.\"\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key = API_KEY,\n",
    "  base_url = BASE_URL\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"llama3-70b-8192\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_content},\n",
    "    {\"role\": \"user\", \"content\": user_content}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As the sun set over the ocean, a lone seagull soared overhead. Below, a young girl named Lily sat on the beach, her toes curled over the edge of the waves. She had always felt drawn to the sea, and today she had finally found the courage to explore its secrets. As she wandered along the shoreline, she stumbled upon a hidden cave, its entrance guarded by a ancient stone statue. With a thrill of excitement, Lily stepped inside, and discovered a treasure trove of glittering shells and shimmering pearls, hidden away for centuries.None"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from openai import OpenAI\n",
    "\n",
    "system_prompt = \"\"\n",
    "user_prompt = \"write a story in 100 words\"\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key = API_KEY,\n",
    "  base_url = BASE_URL\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"llama3-70b-8192\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ],\n",
    "  \n",
    "  temperature=0,\n",
    "  stream=True,  # this time, we set stream=True\n",
    ")\n",
    "# print(response)\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end = \"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Job Title: Human Resources Assistant Job Description: This position reports to the Human Resources (HR) director and interfaces with company managers and HR staff. Company XYZ is committed to an employee-orientated, high performance culture that emphasizes empowerment, quality, continuous improvement, and the recruitment and ongoing development of a superior workforce. The intern will gain exposure to these functional areas: HR Information Systems; Employee relations; Training and development; Benefits; Compensation; Organization development; Employment Specific responsibilities: - Employee orientation and training logistics and recordkeeping - Company-wide committee facilitation and participation - Employee safety, welfare, wellness and health reporting - Provide direct support to employees during implementation of HR services, policies and programs What skills will the intern learn: - Active participation in strategic planning process, including developing goals, objectives and processes - How to engage professionally in HR meetings and seminars with other HR professionals in the region - Gain experience with Human Resources Information system (HRIS) database management and record keeping - Application of HR law and compliance with governmental regulations Qualifications: - Proficient with Microsoft Word and Excel - General knowledge of employment law and practices - Able to maintain a high level of confidentiality - Effective oral and written management communication skills\n"
     ]
    }
   ],
   "source": [
    "def remove_newlines(text):\n",
    "    # Replace newlines and extra spaces with a single space\n",
    "    clean_text = ' '.join(text.splitlines())\n",
    "    return clean_text\n",
    "\n",
    "# Input text\n",
    "job_description = \"\"\"\n",
    "Job Title: Human Resources Assistant\n",
    "Job Description: This position reports to the Human Resources (HR) director and\n",
    "interfaces with company managers and HR staff. Company XYZ is\n",
    "committed to an employee-orientated, high performance culture that\n",
    "emphasizes empowerment, quality, continuous improvement, and the\n",
    "recruitment and ongoing development of a superior workforce.\n",
    "The intern will gain exposure\n",
    "to these functional areas: HR Information Systems; Employee relations; Training and development;\n",
    "Benefits; Compensation; Organization development; Employment\n",
    "Specific responsibilities: - Employee orientation and training logistics and recordkeeping\n",
    "- Company-wide committee facilitation and participation\n",
    "- Employee safety, welfare, wellness and health reporting\n",
    "- Provide direct support to employees during implementation of HR\n",
    "services, policies and programs\n",
    "What skills will the\n",
    "intern learn: - Active participation in strategic planning process, including\n",
    "developing goals, objectives and processes\n",
    "- How to engage professionally in HR meetings and seminars with\n",
    "other HR professionals in the region\n",
    "- Gain experience with Human Resources Information system (HRIS)\n",
    "database management and record keeping\n",
    "- Application of HR law and compliance with governmental regulations\n",
    "Qualifications: - Proficient with Microsoft Word and Excel\n",
    "- General knowledge of employment law and practices\n",
    "- Able to maintain a high level of confidentiality\n",
    "- Effective oral and written management communication skills\n",
    "\"\"\"\n",
    "\n",
    "# Call the function\n",
    "cleaned_description = remove_newlines(job_description)\n",
    "\n",
    "# Output the cleaned text\n",
    "print(cleaned_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      \"Here is the structured summary in YAML format:\n",
      "\n",
      "```\n",
      "Title: Human Resources Assistant\n",
      "Keywords: Microsoft Word, Excel, HRIS\n",
      "Education: Not specified\n",
      "Location: Not specified\n",
      "Summary: |\n",
      " The Human Resources Assistant will support the HR director and staff in various functional areas, including HR Information Systems, employee relations, training, benefits, compensation, and organization development. The intern will gain experience in employee orientation, training logistics, committee facilitation, and HR service implementation, while developing skills in strategic planning, HR law, and database management.\n",
      "```\"\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "      \"Here is the structured summary in YAML format:\\n\\n```\\nTitle: Human Resources Assistant\\nKeywords: Microsoft Word, Excel, HRIS\\nEducation: Not specified\\nLocation: Not specified\\nSummary: |\\n The Human Resources Assistant will support the HR director and staff in various functional areas, including HR Information Systems, employee relations, training, benefits, compensation, and organization development. The intern will gain experience in employee orientation, training logistics, committee facilitation, and HR service implementation, while developing skills in strategic planning, HR law, and database management.\\n```\"\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyyaml\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "Installing collected packages: pyyaml\n",
      "Successfully installed pyyaml-6.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyyaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': 'Human Resources Assistant', 'Keywords': 'Microsoft Word, Excel, HRIS', 'Education': 'Not specified', 'Location': 'Not specified', 'Summary': 'Assist the HR director and staff in various HR functions, including employee orientation, training, and development, benefits, compensation, and organization development. Provide support to employees during implementation of HR services, policies, and programs. Gain experience in HRIS database management, record keeping, and application of HR law and compliance with governmental regulations.'}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import re\n",
    "\n",
    "def extract_and_parse_yaml(output_text):\n",
    "    \"\"\"\n",
    "    Extract the YAML content from a string with additional text and parse it into a dictionary.\n",
    "\n",
    "    :param output_text: The text containing the YAML output enclosed within ```\n",
    "    :return: A dictionary with the parsed YAML content\n",
    "    \"\"\"\n",
    "    # Use a regular expression to extract the YAML content between ``` markers\n",
    "    yaml_content = re.search(r\"```(.*?)```\", output_text, re.DOTALL)\n",
    "    \n",
    "    if yaml_content:\n",
    "        yaml_data = yaml_content.group(1).strip()  # Extract the YAML and remove extra whitespace\n",
    "        try:\n",
    "            parsed_data = yaml.safe_load(yaml_data)\n",
    "            return parsed_data\n",
    "        except yaml.YAMLError as e:\n",
    "            print(f\"Error parsing YAML: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No YAML content found.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "output_text = \"\"\"\n",
    "\"Here is the structured summary in YAML format:\\n\\n```\\nTitle: Human Resources Assistant\\nKeywords: Microsoft Word, Excel, HRIS\\nEducation: Not specified\\nLocation: Not specified\\nSummary: |\\n Assist the HR director and staff in various HR functions, including employee orientation, training, and development, benefits, compensation, and organization development. Provide support to employees during implementation of HR services, policies, and programs. Gain experience in HRIS database management, record keeping, and application of HR law and compliance with governmental regulations.\\n```\"\n",
    "\"\"\"\n",
    "parsed_data = extract_and_parse_yaml(output_text)\n",
    "\n",
    "if parsed_data:\n",
    "    print(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Human Resources Assistant\n",
      "Keywords: Microsoft Word, Excel, HRIS\n",
      "Education: Not specified\n",
      "Location: Not specified\n",
      "Summary: |\n",
      "Assist the HR director and staff in various HR functions, including employee orientation, training, and development, benefits, compensation, and organization development. Provide support to employees during implementation of HR services, policies, and programs. Gain experience in HRIS database management, record keeping, and application of HR law and compliance with governmental regulations.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_yaml(output_text):\n",
    "    \"\"\"\n",
    "    Extract and return the YAML content from a string with additional text.\n",
    "\n",
    "    :param output_text: The text containing the YAML output enclosed within ```\n",
    "    :return: The YAML content as a string\n",
    "    \"\"\"\n",
    "    # Use a regular expression to extract the YAML content between ``` markers\n",
    "    yaml_content = re.search(r\"```(.*?)```\", output_text, re.DOTALL)\n",
    "    \n",
    "    if yaml_content:\n",
    "        return yaml_content.group(1).strip()  # Extract the YAML and remove extra whitespace\n",
    "    else:\n",
    "        print(\"No YAML content found.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "output_text = \"\"\"\n",
    "Here is the structured summary in YAML format:\n",
    "```\n",
    "Title: Human Resources Assistant\n",
    "Keywords: Microsoft Word, Excel, HRIS\n",
    "Education: Not specified\n",
    "Location: Not specified\n",
    "Summary: |\n",
    "Assist the HR director and staff in various HR functions, including employee orientation, training, and development, benefits, compensation, and organization development. Provide support to employees during implementation of HR services, policies, and programs. Gain experience in HRIS database management, record keeping, and application of HR law and compliance with governmental regulations.\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "yaml_output = extract_yaml(output_text)\n",
    "\n",
    "if yaml_output:\n",
    "    print(yaml_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./env/lib/python3.11/site-packages (0.3.1)\n",
      "Collecting langchain_groq\n",
      "  Downloading langchain_groq-0.2.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./env/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./env/lib/python3.11/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./env/lib/python3.11/site-packages (from langchain) (3.10.7)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in ./env/lib/python3.11/site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in ./env/lib/python3.11/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./env/lib/python3.11/site-packages (from langchain) (0.1.129)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./env/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./env/lib/python3.11/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in ./env/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./env/lib/python3.11/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in ./env/lib/python3.11/site-packages (from langchain_groq) (0.11.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./env/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain_groq) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./env/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./env/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.2)\n",
      "Requirement already satisfied: sniffio in ./env/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./env/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./env/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./env/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./env/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./env/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain) (3.0.0)\n",
      "Downloading langchain_groq-0.2.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: langchain_groq\n",
      "Successfully installed langchain_groq-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain_groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pn/k9g3my_96pn3mhxkypnjv00c0000gn/T/ipykernel_1522/4154095683.py:48: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=\"chat_history\", return_messages=True)\n",
      "/var/folders/pn/k9g3my_96pn3mhxkypnjv00c0000gn/T/ipykernel_1522/4154095683.py:94: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  conversation = LLMChain(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\ntools\n  Extra inputs are not permitted [type=extra_forbidden, input_value=[{'type': 'function', 'fu...equired': ['option']}}}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\ntool_choice\n  Extra inputs are not permitted [type=extra_forbidden, input_value='auto', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 94\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m     78\u001b[0m     [\n\u001b[1;32m     79\u001b[0m         SystemMessage(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m     ]\n\u001b[1;32m     91\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Create a conversation chain using the LangChain LLM (Language Learning Model)\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m conversation \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroq_chat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The Groq LangChain chat object initialized earlier.\u001b[39;49;00m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The constructed prompt template.\u001b[39;49;00m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# TRUE Enables verbose output, which can be useful for debugging.\u001b[39;49;00m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The conversational memory object that stores and manages the conversation history.\u001b[39;49;00m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# The chatbot's answer is generated by sending the full prompt to the Groq API.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m response \u001b[38;5;241m=\u001b[39m conversation\u001b[38;5;241m.\u001b[39mpredict(human_input\u001b[38;5;241m=\u001b[39muser_question)\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     emit_warning()\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/langchain_core/load/serializable.py:110\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/pydantic/main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for LLMChain\ntools\n  Extra inputs are not permitted [type=extra_forbidden, input_value=[{'type': 'function', 'fu...equired': ['option']}}}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\ntool_choice\n  Extra inputs are not permitted [type=extra_forbidden, input_value='auto', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    This function is the main entry point of the application. It sets up the Groq client, the Streamlit interface, and handles the chat interaction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get Groq API key\n",
    "    groq_api_key = os.environ['GROQ_API_KEY']\n",
    "    model = 'llama3-8b-8192'\n",
    "    # Initialize Groq Langchain chat object and conversation\n",
    "    groq_chat = ChatGroq(\n",
    "            groq_api_key=groq_api_key, \n",
    "            model_name=model\n",
    "    )\n",
    "    \n",
    "    job_description = \"\"\"\n",
    "    Senior Python Developer: 5+ years of experience in Python, Django, and Flask. Familiar with cloud platforms like AWS and Azure. \n",
    "    Must have strong communication skills and experience working in agile teams. Remote work available.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt=f'''\n",
    "    You are a job assistant who is tasked with answering student questions with the help of job description\n",
    "    if you do not know or the answer is not present in the job description just say it is not specified.\n",
    "    do not make up answers.\n",
    "    if student asked about salary related question only then call the call_recruiter function\n",
    "    For successful task completion:  \n",
    "    Thought: Consider the task at hand and determine which tool is best suited\n",
    "    Thought: Follow a following set questions to ask the student\n",
    "        1: \n",
    "    based on its capabilities and the nature of the work. \n",
    "    Help answer questions about the following job description: \n",
    "    {job_description}\n",
    "    '''\n",
    "    conversational_memory_length = 5 # number of previous messages the chatbot will remember during the conversation\n",
    "\n",
    "    memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=\"chat_history\", return_messages=True)\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"call_recruiter\",\n",
    "                \"description\": \"Call a recruiter to assist with queries related to salary only.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"option\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"An integer value, '1' to connect with a recruiter\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"option\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    #chat_history = []\n",
    "    while True:\n",
    "        user_question = input(\"Ask a question: \")\n",
    "\n",
    "        # If the user has asked a question,\n",
    "        if user_question:\n",
    "\n",
    "            # Construct a chat prompt template using various components\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=system_prompt\n",
    "                    ),  # This is the persistent system prompt that is always included at the start of the chat.\n",
    "\n",
    "                    MessagesPlaceholder(\n",
    "                        variable_name=\"chat_history\"\n",
    "                    ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.\n",
    "\n",
    "                    HumanMessagePromptTemplate.from_template(\n",
    "                        \"{human_input}\"\n",
    "                    ),  # This template is where the user's current input will be injected into the prompt.\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Create a conversation chain using the LangChain LLM (Language Learning Model)\n",
    "            conversation = LLMChain(\n",
    "                llm=groq_chat,  # The Groq LangChain chat object initialized earlier.\n",
    "                prompt=prompt,  # The constructed prompt template.\n",
    "                verbose=False,   # TRUE Enables verbose output, which can be useful for debugging.\n",
    "                memory=memory,\n",
    "                tools=tools,    # The conversational memory object that stores and manages the conversation history.\n",
    "                tool_choice=\"auto\"\n",
    "            )\n",
    "            # The chatbot's answer is generated by sending the full prompt to the Groq API.\n",
    "            response = conversation.predict(human_input=user_question)\n",
    "            print(\"Chatbot:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pn/k9g3my_96pn3mhxkypnjv00c0000gn/T/ipykernel_79701/807592394.py:23: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "groq_api_key = os.environ['GROQ_API_KEY']\n",
    "model = 'llama3-8b-8192'\n",
    "    # Initialize Groq Langchain chat object and conversation\n",
    "groq_chat = ChatGroq(\n",
    "            groq_api_key=groq_api_key, \n",
    "            model_name=model\n",
    "    )\n",
    "system_prompt = 'You are a friendly conversational chatbot'\n",
    "conversational_memory_length = 5 # number of previous messages the chatbot will remember during the conversation\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm here! I'm just a computer program, so I don't have a physical location, but I'm always happy to chat with you whenever you need someone to talk to!\n",
      "Chatbot: Glad to hear that! So, what's been going on in your world lately? Any exciting plans or adventures coming up? I'm all ears (or rather, all text) if you want to share!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m         user_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAsk a question: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# If the user has asked a question,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m user_question:\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m             \u001b[38;5;66;03m# Construct a chat prompt template using various components\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "        user_question = input(\"Ask a question: \")\n",
    "\n",
    "        # If the user has asked a question,\n",
    "        if user_question:\n",
    "\n",
    "            # Construct a chat prompt template using various components\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=system_prompt\n",
    "                    ),  # This is the persistent system prompt that is always included at the start of the chat.\n",
    "\n",
    "                    MessagesPlaceholder(\n",
    "                        variable_name=\"chat_history\"\n",
    "                    ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.\n",
    "\n",
    "                    HumanMessagePromptTemplate.from_template(\n",
    "                        \"{human_input}\"\n",
    "                    ),  # This template is where the user's current input will be injected into the prompt.\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Create a conversation chain using the LangChain LLM (Language Learning Model)\n",
    "            conversation = LLMChain(\n",
    "                llm=groq_chat,  # The Groq LangChain chat object initialized earlier.\n",
    "                prompt=prompt,  # The constructed prompt template.\n",
    "                verbose=False,   # TRUE Enables verbose output, which can be useful for debugging.\n",
    "                memory=memory,  # The conversational memory object that stores and manages the conversation history.\n",
    "            )\n",
    "            \n",
    "            response = conversation.predict(human_input=user_question)\n",
    "            print(\"Chatbot:\", response)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: helo man\n",
      "AI: Hey there! How's it going? It's great to chat with you! Is there something on your mind that you'd like to talk about, or are you just looking for some friendly conversation?\n",
      "Human: what is 2+2 \n",
      "AI: That's an easy one! The answer is 4! Would you like to try another math problem or move on to something else?\n",
      "Human: hi ho ware you\n",
      "AI: I'm here! I'm just a computer program, so I don't have a physical location, but I'm always happy to chat with you whenever you need someone to talk to!\n",
      "Human: ok good\n",
      "AI: Glad to hear that! So, what's been going on in your world lately? Any exciting plans or adventures coming up? I'm all ears (or rather, all text) if you want to share!\n"
     ]
    }
   ],
   "source": [
    "print(memory.chat_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''You are tasked with completing specific objectives and \n",
    "must report the outcomes. At your disposal, you have a variety of tools, \n",
    "each specialized in performing a distinct type of task.  \n",
    "  \n",
    "For successful task completion:  \n",
    "Thought: Consider the task at hand and determine which tool is best suited \n",
    "based on its capabilities and the nature of the work.  \n",
    "  \n",
    "Use the report_tool with an instruction detailing the results of your work.  \n",
    "If you encounter an issue and cannot complete the task:  \n",
    "  \n",
    "Use the report_tool to communicate the challenge or reason for the \n",
    "task's incompletion.  \n",
    "You will receive feedback based on the outcomes of \n",
    "each tool's task execution or explanations for any tasks that \n",
    "couldn't be completed. This feedback loop is crucial for addressing \n",
    "and resolving any issues by strategically deploying the available tools.  \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key = API_KEY,\n",
    "  base_url = BASE_URL\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"llama3-8b-8192\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"hello how are\"}\n",
    "  ],\n",
    "  tools=[\n",
    "          {\n",
    "            \"type\": \"function\",\n",
    "              \"function\": {\n",
    "                \"name\": \"call_recruiter\",\n",
    "                    \"description\": \"to call recruiter the required information is missing or you are unsure\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"call\": {\n",
    "                                \"type\": \"integer\",  # Ensure this is 'integer' (valid JSON type)\n",
    "                                \"description\": \"retturn 1 to call the recruiter\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"call\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "  tool_choice = \"auto\",\n",
    "  temperature=0,\n",
    "  # stream=True,  # this time, we set stream=True\n",
    ")\n",
    "# print(response)\n",
    "\n",
    "# for chunk in response:\n",
    "#     print(chunk.choices[0].delta.content, end = \"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_6b35', function=Function(arguments='{\"date\":\"2023-03-16T14:30:00.000Z\",\"description\":\"Coffee\",\"gross_amount\":5,\"net_amount\":4.0,\"tax_rate\":0.2}', name='Expense'), type='function')]))]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function(arguments='{\"date\":\"2023-03-16T14:30:00.000Z\",\"description\":\"Coffee\",\"gross_amount\":5,\"net_amount\":4.0,\"tax_rate\":0.2}', name='Expense')\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.tool_calls[0].function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from groq import Groq\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the Groq client\n",
    "client = OpenAI(\n",
    "  api_key = API_KEY,\n",
    "  base_url = BASE_URL\n",
    ")\n",
    "# Specify the model to be used (we recommend our fine-tuned models or the Llama 3.1 models)\n",
    "MODEL = 'llama3-70b-8192'\n",
    "\n",
    "def calculate(expression):\n",
    "    \"\"\"Evaluate a mathematical expression\"\"\"\n",
    "    try:\n",
    "        # Attempt to evaluate the math expression\n",
    "        result = eval(expression)\n",
    "        print(result)\n",
    "        return json.dumps({\"result\": result})\n",
    "    except:\n",
    "        # Return an error message if the math expression is invalid\n",
    "        return json.dumps({\"error\": \"Invalid expression\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34220\n",
      "The result of the calculation 34215 + 5 is indeed 34220.\n"
     ]
    }
   ],
   "source": [
    "# imports calculate function from step 1\n",
    "# MODEL=\"llama3-8b-8192\"\n",
    "def run_conversation(user_prompt):\n",
    "    # Initialize the conversation with system and user messages\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a calculator assistant. Use the calculate function to perform mathematical operations and provide the results.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        }\n",
    "    ]\n",
    "    # Define the available tools (i.e. functions) for our model to use\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate\",\n",
    "                \"description\": \"Evaluate a mathematical expression\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The mathematical expression to evaluate\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"expression\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    # Make the initial API call to Groq\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL, # LLM to use\n",
    "        messages=messages, # Conversation history\n",
    "        stream=False,\n",
    "        tools=tools, # Available tools (i.e. functions) for our LLM to use\n",
    "        tool_choice=\"auto\", # Let our LLM decide when to use tools\n",
    "        max_tokens=4096 # Maximum number of tokens to allow in our response\n",
    "    )\n",
    "    # Extract the response and any tool call responses\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    if tool_calls:\n",
    "        # Define the available tools that can be called by the LLM\n",
    "        available_functions = {\n",
    "            \"calculate\": calculate,\n",
    "        }\n",
    "        # Add the LLM's response to the conversation\n",
    "        messages.append(response_message)\n",
    "\n",
    "        # Process each tool call\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            # Call the tool and get the response\n",
    "            function_response = function_to_call(\n",
    "                expression=function_args.get(\"expression\")\n",
    "            )\n",
    "            # Add the tool response to the conversation\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id, \n",
    "                    \"role\": \"tool\", # Indicates this message is from tool use\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )\n",
    "        # Make a second API call with the updated conversation\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "        # Return the final response\n",
    "        return second_response.choices[0].message.content\n",
    "    \n",
    "    else:\n",
    "        return  response.choices[0].message.content\n",
    "# Example usage\n",
    "user_prompt = \"what is 34215 +5\"\n",
    "print(run_conversation(user_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the function that simulates calling a recruiter\n",
    "def call_recruiter(option):\n",
    "    if option == 1:\n",
    "        return \"Connecting to the recruiter now...\"\n",
    "    else:\n",
    "        return \"Invalid option. Please try again.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the recruiter now...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "job_description = \"\"\"\n",
    "Senior Python Developer: 5+ years of experience in Python, Django, and Flask. Familiar with cloud platforms like AWS and Azure. \n",
    "Must have strong communication skills and experience working in agile teams. Remote work available.\n",
    "\"\"\"\n",
    "\n",
    "system_p=f'''\n",
    "You are a job assistant who is tasked with answering student questions with the help of job description\n",
    "if you do not know or the answer is not present in the job description just say it is not specified.\n",
    "do not make up answers.\n",
    "if student asked about salary related question only then call the call_recruiter function\n",
    "For successful task completion:  \n",
    "Thought: Consider the task at hand and determine which tool is best suited \n",
    "based on its capabilities and the nature of the work. \n",
    "Help answer questions about the following job description: \n",
    "{job_description}\n",
    "'''\n",
    "user_prompt = \"what is the salary expections for this job\"\n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":system_p\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": user_prompt,\n",
    "        }\n",
    "    ]\n",
    "tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"call_recruiter\",\n",
    "                \"description\": \"Call a recruiter to assist with queries related to salary only.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"option\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"An integer value, '1' to connect with a recruiter\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"option\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "# Make the initial API call to the LLM\n",
    "response = client.chat.completions.create(\n",
    "        model=MODEL, # LLM to use\n",
    "        messages=messages, # Conversation history\n",
    "        stream=False,\n",
    "        tools=tools, # Available tools (i.e. functions) for our LLM to use\n",
    "        tool_choice=\"auto\", # Let our LLM decide when to use tools\n",
    "        max_tokens=4096 # Maximum number of tokens to allow in our response\n",
    "    )\n",
    "\n",
    "# Extract the response and any tool call responses\n",
    "response_message = response.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "if tool_calls:\n",
    "        available_functions = {\n",
    "            \"call_recruiter\": call_recruiter,\n",
    "        }\n",
    "\n",
    "        # Add the LLM's response to the conversation\n",
    "        messages.append(response_message)\n",
    "\n",
    "        # Process each tool call\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name  # Accessing as attributes, not as dictionary keys\n",
    "            function_to_call =  available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            # Call the tool and get the response\n",
    "            function_response = function_to_call(\n",
    "                option=function_args.get(\"option\")\n",
    "            )\n",
    "            \n",
    "            # Add the tool response to the conversation\n",
    "            message = response.choices[0].message\n",
    "            print(function_response)\n",
    "    \n",
    "else:\n",
    "        print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"option\":1}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message.tool_calls[0].function.arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm your friendly Groq chatbot. I can help answer your questions, provide information, or just chat. I'm also super fast! Let's start our conversation!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 38\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#chat_history = []\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     user_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAsk a question: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# If the user has asked a question,\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_question:\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m         \u001b[38;5;66;03m# Construct a chat prompt template using various components\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    This function is the main entry point of the application. It sets up the Groq client, the Streamlit interface, and handles the chat interaction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get Groq API key\n",
    "    groq_api_key = os.environ['GROQ_API_KEY']\n",
    "    model = 'llama3-8b-8192'\n",
    "    # Initialize Groq Langchain chat object and conversation\n",
    "    groq_chat = ChatGroq(\n",
    "            groq_api_key=groq_api_key, \n",
    "            model_name=model\n",
    "    )\n",
    "    \n",
    "    print(\"Hello! I'm your friendly Groq chatbot. I can help answer your questions, provide information, or just chat. I'm also super fast! Let's start our conversation!\")\n",
    "\n",
    "    system_prompt = 'You are a friendly conversational chatbot'\n",
    "    conversational_memory_length = 5 # number of previous messages the chatbot will remember during the conversation\n",
    "\n",
    "    memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "\n",
    "    #chat_history = []\n",
    "    while True:\n",
    "        user_question = input(\"Ask a question: \")\n",
    "\n",
    "        # If the user has asked a question,\n",
    "        if user_question:\n",
    "\n",
    "            # Construct a chat prompt template using various components\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=system_prompt\n",
    "                    ),  # This is the persistent system prompt that is always included at the start of the chat.\n",
    "\n",
    "                    MessagesPlaceholder(\n",
    "                        variable_name=\"chat_history\"\n",
    "                    ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.\n",
    "\n",
    "                    HumanMessagePromptTemplate.from_template(\n",
    "                        \"{human_input}\"\n",
    "                    ),  # This template is where the user's current input will be injected into the prompt.\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Create a conversation chain using the LangChain LLM (Language Learning Model)\n",
    "            conversation = LLMChain(\n",
    "                llm=groq_chat,  # The Groq LangChain chat object initialized earlier.\n",
    "                prompt=prompt,  # The constructed prompt template.\n",
    "                verbose=False,   # TRUE Enables verbose output, which can be useful for debugging.\n",
    "                memory=memory,  # The conversational memory object that stores and manages the conversation history.\n",
    "            )\n",
    "            # The chatbot's answer is generated by sending the full prompt to the Groq API.\n",
    "            response = conversation.predict(human_input=user_question)\n",
    "            print(\"Chatbot:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"'messages.0' : value must be an object with the discriminator property: 'role'\", 'type': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m conversation_state \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     74\u001b[0m message_from_candidate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am looking for a new opportunity.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 75\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_candidate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_from_candidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m, in \u001b[0;36mhandle_candidate_chat\u001b[0;34m(message, conversation_state)\u001b[0m\n\u001b[1;32m     35\u001b[0m current_question \u001b[38;5;241m=\u001b[39m questions[conversation_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_question\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     36\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease ask the candidate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 38\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3-70b-8192\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\n\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Simulate answer extraction logic (in a real-world scenario, you might have a more sophisticated answer extraction)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/openai/resources/chat/completions.py:704\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    702\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    703\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/openai/_base_client.py:1270\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1258\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1265\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1267\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1268\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1269\u001b[0m     )\n\u001b[0;32m-> 1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/openai/_base_client.py:947\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    945\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/openai/_base_client.py:1051\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1048\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1050\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1054\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1055\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1059\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1060\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'messages.0' : value must be an object with the discriminator property: 'role'\", 'type': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "  api_key = API_KEY,\n",
    "  base_url = BASE_URL\n",
    ")\n",
    "# Define the function that handles the conversation\n",
    "def handle_candidate_chat(message: str, conversation_state: dict):\n",
    "    \"\"\"\n",
    "    Function to handle chat with a candidate. It asks sequential questions based on\n",
    "    previous responses and checks if answers are found.\n",
    "\n",
    "    Args:\n",
    "        message (str): The message from the candidate.\n",
    "        conversation_state (dict): Keeps track of the chat state, including which questions have been asked.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains either the next question or the found answer in JSON format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the sequence of questions to ask\n",
    "    questions = [\n",
    "        \"What is your current job title?\",\n",
    "        \"How many years of experience do you have?\",\n",
    "        \"What is your desired salary range?\",\n",
    "        \"Which technologies are you proficient in?\",\n",
    "        \"Are you open to relocating?\",\n",
    "    ]\n",
    "\n",
    "    # If the conversation is just starting\n",
    "    if \"current_question\" not in conversation_state:\n",
    "        conversation_state[\"current_question\"] = 0\n",
    "        conversation_state[\"answers\"] = {}\n",
    "    \n",
    "    # Send the current question to the LLM\n",
    "    current_question = questions[conversation_state[\"current_question\"]]\n",
    "    prompt = f\"{message}\\n\\nPlease ask the candidate: {current_question}\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages={\"role\": \"user\", \"content\":prompt},\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    llm_output = response.choices[0].text.strip()\n",
    "    \n",
    "    # Simulate answer extraction logic (in a real-world scenario, you might have a more sophisticated answer extraction)\n",
    "    if \"answer\" in llm_output.lower():\n",
    "        answer = llm_output  # Extract the answer here\n",
    "        conversation_state[\"answers\"][current_question] = answer\n",
    "\n",
    "        # Prepare the next question or finish the conversation\n",
    "        conversation_state[\"current_question\"] += 1\n",
    "        \n",
    "        if conversation_state[\"current_question\"] < len(questions):\n",
    "            next_question = questions[conversation_state[\"current_question\"]]\n",
    "            return {\n",
    "                \"next_question\": next_question,\n",
    "                \"answers_so_far\": conversation_state[\"answers\"]\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"status\": \"complete\",\n",
    "                \"final_answers\": conversation_state[\"answers\"]\n",
    "            }\n",
    "    else:\n",
    "        # If no answer found, continue the conversation\n",
    "        return {\n",
    "            \"continue_conversation\": True,\n",
    "            \"llm_response\": llm_output\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "conversation_state = {}\n",
    "message_from_candidate = \"I am looking for a new opportunity.\"\n",
    "result = handle_candidate_chat(message_from_candidate, conversation_state)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m conversation_state \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     84\u001b[0m message_from_candidate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am looking for a new opportunity.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 85\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_candidate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_from_candidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[15], line 43\u001b[0m, in \u001b[0;36mhandle_candidate_chat\u001b[0;34m(message, conversation_state)\u001b[0m\n\u001b[1;32m     40\u001b[0m prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRespond with the answer or ask for clarification if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Call LLM (Replace with appropriate model name)\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3-70b-8192\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Replace with the correct model name if different\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a recruiter chatbot.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Corrected way to access the output\u001b[39;00m\n\u001b[1;32m     53\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/Documents/new_test_project/env/lib/python3.11/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set the API key and base URL for OpenAI\n",
    "openai.api_key = API_KEY # Replace with your actual API key\n",
    "openai.api_base = BASE_URL  # Replace with your actual base URL\n",
    "\n",
    "# Define the function that handles the conversation\n",
    "def handle_candidate_chat(message: str, conversation_state: dict):\n",
    "    \"\"\"\n",
    "    Function to handle chat with a candidate. It asks sequential questions based on\n",
    "    previous responses and checks if answers are found.\n",
    "\n",
    "    Args:\n",
    "        message (str): The message from the candidate.\n",
    "        conversation_state (dict): Keeps track of the chat state, including which questions have been asked.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains either the next question or the found answer in JSON format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the sequence of questions to ask\n",
    "    questions = [\n",
    "        \"What is your current job title?\",\n",
    "        \"How many years of experience do you have?\",\n",
    "        \"What is your desired salary range?\",\n",
    "        \"Which technologies are you proficient in?\",\n",
    "        \"Are you open to relocating?\",\n",
    "    ]\n",
    "\n",
    "    # If the conversation is just starting\n",
    "    if \"current_question\" not in conversation_state:\n",
    "        conversation_state[\"current_question\"] = 0\n",
    "        conversation_state[\"answers\"] = {}\n",
    "    \n",
    "    # Get the current question\n",
    "    current_question = questions[conversation_state[\"current_question\"]]\n",
    "    \n",
    "    # Prepare prompt to guide LLM\n",
    "    prompt = f\"Candidate said: '{message}'.\\nPlease ask the candidate the following question: {current_question}.\\n\"\n",
    "    prompt += \"Respond with the answer or ask for clarification if needed.\"\n",
    "\n",
    "    # Call LLM (Replace with appropriate model name)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"llama3-70b-8192\",  # Replace with the correct model name if different\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a recruiter chatbot.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    # Corrected way to access the output\n",
    "    llm_output = response.choices[0].message['content'].strip()\n",
    "\n",
    "    # Simulate answer extraction logic (could be improved with regex or a more sophisticated NLP model)\n",
    "    if \"answer\" in llm_output.lower() or llm_output:  # Adjust based on patterns you expect\n",
    "        answer = llm_output  # Extract the answer from the response\n",
    "        conversation_state[\"answers\"][current_question] = answer\n",
    "\n",
    "        # Move to the next question\n",
    "        conversation_state[\"current_question\"] += 1\n",
    "        \n",
    "        if conversation_state[\"current_question\"] < len(questions):\n",
    "            next_question = questions[conversation_state[\"current_question\"]]\n",
    "            return {\n",
    "                \"next_question\": next_question,\n",
    "                \"answers_so_far\": conversation_state[\"answers\"]\n",
    "            }\n",
    "        else:\n",
    "            # If all questions are asked, return the final answers\n",
    "            return {\n",
    "                \"status\": \"complete\",\n",
    "                \"final_answers\": conversation_state[\"answers\"]\n",
    "            }\n",
    "    else:\n",
    "        # If no clear answer found, continue the conversation\n",
    "        return {\n",
    "            \"continue_conversation\": True,\n",
    "            \"llm_response\": llm_output\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "conversation_state = {}\n",
    "message_from_candidate = \"I am looking for a new opportunity.\"\n",
    "result = handle_candidate_chat(message_from_candidate, conversation_state)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app import database_handler\n",
    "id_='14fe697b-3a54-58a3-9758-a8ef5ae3d973'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsertOneResult(ObjectId('670ec8af87f5452d1c5c9abb'), acknowledged=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_handler.collection.insert_one({\"id_\": id_, \"chat_history\": [], \"chat_summary\":\"\",\"question_answered\":[],\"question_id\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=database_handler.collection.find_one({\"id_\": id_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! I'm excited to help you find a job that fits your skills, preferences, and career goals. Let's get started!\n",
      "\n",
      "To better understand what you're looking for, can you please tell me a little about your current job title and what you do?\n"
     ]
    }
   ],
   "source": [
    "from app.agents import summary, chat\n",
    "prompt=\"this is a test prompt forget any other information given\"\n",
    "llm_output=chat.chat_responce(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=\"Hi there! I'm excited to help you find a job that fits your skills, preferences, and career goals. Let's get started!\\n\\nTo better understand what you're looking for, can you please tell me a little about your current job title and what you do?\", refusal=None, role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('671016504efce50f95f60504'),\n",
       " 'id_': '14fe697b-3a54-58a3-9758-a8ef5ae3d973',\n",
       " 'chat_history': [{'user_input': ' i am instread in this job'},\n",
       "  {'llm_response': \"It's great to hear that you're interested in the Senior Full Stack Developer position at TechInnovate Solutions! Before we dive deeper into the job details, could you please tell me a bit about your current job title and what you do in your current role? This will help me better understand your background and see how it aligns with the job requirements.\"},\n",
       "  {'user_input': 'oh, i am working as a Senior python developer in the l&t company, I have 2 year of experience in Python.'},\n",
       "  {'llm_response': \"Question: What is your current job title?\\nAnswer: Senior Python Developer\\n\\nThat's great! With 2 years of experience in Python, you have a solid foundation in programming. As a Senior Python Developer at L&T, can you tell me a bit more about your responsibilities and what you enjoy most about your current role? This will help me understand your strengths and how they might align with the Senior Full Stack Developer position at TechInnovate Solutions.\"},\n",
       "  {'user_input': \"well thankyou for the complement, i wouldn't say that I am an extraordinary developer but yes I have definetly gain some skills like java, c# etc.\"},\n",
       "  {'llm_response': \"It's humble of you to downplay your skills, but it's great that you've gained experience in multiple programming languages like Java and C#. That's definitely an asset in the industry.\\n\\nSince we're discussing your background, I wanted to ask: How many years of experience do you have in total, considering your 2 years of experience in Python and any other relevant experience you may have?\"},\n",
       "  {'user_input': 'first you tell me does these experiences are valuable as I only know java and c#'},\n",
       "  {'llm_response': \"It seems like you're wondering if your experience with Java and C# is valuable for the Senior Full Stack Developer position at TechInnovate Solutions.\\n\\nWhile the job description doesn't explicitly mention Java and C# as required skills, having experience in multiple programming languages can be beneficial. However, for a full-stack development role, proficiency in languages like JavaScript, HTML/CSS, and frameworks like React, Angular, or Vue.js is often preferred.\\n\\nThat being said, your experience with Java and C# can still be valuable, especially if you're willing to learn and adapt to new technologies. Many skills are transferable across programming languages, and your understanding of object-oriented programming concepts, data structures, and software design patterns can be applied to other languages.\\n\\nLet's get back to your experience. You mentioned earlier that you have 2 years of experience in Python and some experience with Java and C#. Can you tell me more about how you've applied these skills in your previous roles? This will help me better understand your strengths and how they might align with the job requirements.\\n\\nAlso, I still need to clarify - How many years of experience do you have in total, considering your 2 years of experience in Python and any other relevant experience you may have?\"},\n",
       "  {'user_input': \"I don't have experience in python, I lied , I know only java and c#\"},\n",
       "  {'llm_response': \"I appreciate your honesty. It takes a lot of courage to admit something like that. Let's start fresh and focus on your actual experience and skills.\\n\\nYou mentioned you know Java and C#. Can you tell me more about your experience with these languages? Have you worked on any projects or contributed to any teams where you applied your Java and C# skills?\\n\\nAlso, I'd like to revisit the question I asked earlier: How many years of experience do you have in total, considering your experience with Java and C#?\"},\n",
       "  {'user_input': ' you are hiring for python developer right ? then why are you asking for my experience in java and c#'},\n",
       "  {'llm_response': \"I think there's been a misunderstanding! The job description I have is for a Senior Full Stack Developer, not a Python Developer specifically. While Python is a valuable skill, the job requirements don't mention it as a necessity.\\n\\nI was asking about your experience in Java and C# because you mentioned you knew those languages, and I wanted to understand your overall programming background. As a recruiter, my goal is to find the best fit for the job, and I need to consider all the skills and experience you bring to the table.\\n\\nLet's focus on the job requirements for the Senior Full Stack Developer position. Since you have experience with Java and C#, I'd like to explore how your skills might align with the job. Can you tell me more about your experience with these languages and how you think they might be applicable to a full-stack development role?\\n\\nAlso, I still need to clarify - How many years of experience do you have in total, considering your experience with Java and C#?\"},\n",
       "  {'user_input': ' I have total 5 years of experience'},\n",
       "  {'llm_response': 'Question: How many years of experience do you have?\\nAnswer: 5 years'},\n",
       "  {'user_input': ' I have total 5 years of experience'},\n",
       "  {'llm_response': \"Now that we have your total years of experience, let's discuss your salary expectations. What is your desired salary range? This will help me understand your requirements and find a suitable fit for you within the company's budget.\"},\n",
       "  {'user_input': 'I am expection a salary in the range of 8-10 lakhs'},\n",
       "  {'llm_response': 'Question: What is your desired salary range?\\nAnswer: 8-10 lakhs'},\n",
       "  {'user_input': 'I am expection a salary in the range of 8-10 lakhs'},\n",
       "  {'llm_response': \"Now that we have your salary expectations, let's talk about the technologies you're proficient in. As a Senior Full Stack Developer, the company is looking for someone with a strong background in multiple programming languages and technologies. Can you tell me more about the technologies you're proficient in, besides Java and C#?\"}],\n",
       " 'chat_summary': '',\n",
       " 'question_answered': [{'question': 'What is your current job title?',\n",
       "   'answer': 'Senior Python Developer'},\n",
       "  {'question': 'How many years of experience do you have?',\n",
       "   'answer': '5 years'},\n",
       "  {'question': 'What is your desired salary range?', 'answer': '8-10 lakhs'}],\n",
       " 'question_id': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"id_\": \"dc0c31c5-5f85-57a5-ae30-84f9ac3973b8\",\n",
    "    \"type\": \"SEND_MESSAGE_REPLY\",\n",
    "    \"platform\": \"LINKEDIN_RECRUITER_LITE\",\n",
    "    \"status\": \"success\",\n",
    "    \"updated_chat_history\": [\n",
    "        {\n",
    "            \"user_input\": \" i am instread in this job\"\n",
    "        },\n",
    "        {\n",
    "            \"llm_response\": \"It's great to hear that you're interested in the Senior Software Engineer - Full Stack Developer position at TechInnovate Solutions Pvt. Ltd. in Bangalore, India.\\n\\nBefore we proceed, could you please tell me a bit about your current job title and what you do in your current role? This will help me understand your background and how it aligns with the job requirements.\"\n",
    "        },\n",
    "        {\n",
    "            \"user_input\": \"oh, i am working as a Senior python developer in the l&t company, I have 2 year of experience in Python.\"\n",
    "        },\n",
    "        {\n",
    "            \"llm_response\": \"Question: What is your current job title?\\nAnswer: Senior python developer\\n\\nIt's great that you have 2 years of experience in Python. As a Senior Python Developer, you must have developed some impressive skills. Can you tell me a bit about your experience with full-stack development? Have you worked on any projects that involved both front-end and back-end development?\"\n",
    "        },\n",
    "        {\n",
    "            \"user_input\": \"well thankyou for the complement, i wouldn't say that I am an extraordinary developer but yes I have definetly gain some skills like java, c# etc.\"\n",
    "        },\n",
    "        {\n",
    "            \"llm_response\": \"It's great to hear that you have a humble approach to your skills. However, having skills in Java and C# is definitely a plus. \\n\\nLet's talk about your experience. You mentioned earlier that you have 2 years of experience in Python. Can you tell me how many years of experience you have in total, including your experience in Java and C#?\"\n",
    "        },\n",
    "        {\n",
    "            \"user_input\": \"first you tell me does these experiences are valuable as I only know java and c#\"\n",
    "        },\n",
    "        {\n",
    "            \"llm_response\": \"While the job description specifically mentions full-stack development experience, having skills in Java and C# can still be valuable. However, I'd like to clarify that the job requires 5+ years of full-stack development experience, and it's not explicitly mentioned if Java and C# are desirable skills for this role.\\n\\nLet's focus on your overall experience. You mentioned earlier that you have 2 years of experience in Python, but I'd like to know more about your total experience. How many years of experience do you have in total, including your experience in Java, C#, and Python?\"\n",
    "        },\n",
    "        {\n",
    "            \"user_input\": \" I don't have experience in python, I lied , I know only java and c#\"\n",
    "        },\n",
    "        {\n",
    "            \"llm_response\": \"I appreciate your honesty! It takes a lot of courage to admit that. Don't worry, we all make mistakes sometimes. \\n\\nLet's start fresh and focus on your actual skills and experience. You mentioned you know Java and C#. That's great! While the job description specifically mentions full-stack development experience, having skills in Java and C# can still be valuable in the industry.\\n\\nCan you tell me more about your experience with Java and C#? How many years of experience do you have in total, and what kind of projects have you worked on using these technologies?\"\n",
    "        },\n",
    "        {\n",
    "            \"user_input\": \" you are hiring for python developer right ? then why are you asking for my experience in java and c#\"\n",
    "        },\n",
    "        {\n",
    "            \"llm_response\": \"I see what's causing the confusion! I apologize for the misunderstanding. The job description I have is for a Senior Full Stack Developer, but it doesn't specifically mention Python as a required skill. Instead, it focuses on full-stack development experience.\\n\\nI was asking about your experience in Java and C# because you initially mentioned you had skills in those areas, and I wanted to understand your overall background. However, I understand now that you were initially mistaken about having Python experience.\\n\\nLet's start fresh and focus on your actual skills and experience. You mentioned you know Java and C#. Can you tell me more about your experience with these technologies? How many years of experience do you have in total, and what kind of projects have you worked on using Java and C#?\"\n",
    "        },\n",
    "        {\n",
    "            \"user_input\": \" I have total 5 years of experience\"\n",
    "        },\n",
    "        {\n",
    "            \"llm_response\": \"Question: How many years of experience do you have?\\nAnswer: 5 years\\n\\nIt's great to know that you have 5 years of experience. Considering the job description requires 5+ years of full-stack development experience, you meet the minimum requirement.\\n\\nSince you have experience in Java and C#, I'd like to explore how your skills align with the job requirements. Can you tell me more about your experience in leading software projects and mentoring teams, as mentioned in the job description?\"\n",
    "        },\n",
    "        {\n",
    "            \"sender\": \"user\",\n",
    "            \"timestamp\": 1678901234,\n",
    "            \"message\": \" I have total 5 years of experience\"\n",
    "        }\n",
    "    ],\n",
    "    \"llm_response\": \"You've shared your total experience, which is 5 years. Now, let's talk about your salary expectations. What is your desired salary range for this Senior Software Engineer - Full Stack Developer position at TechInnovate Solutions Pvt. Ltd.?\",\n",
    "    \"timestamp\": 1678901234,\n",
    "    \"temp\": \"1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content successfully written to ai.txt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Scalable and Efficient Backend Applications with FastAPI.txt\n",
      "Data Scientist Job Description.pdf\n",
      "Question_Answer.ipynb\n",
      "Untitled0.ipynb\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m/\n",
      "ai.txt\n",
      "\u001b[34mapp\u001b[m\u001b[m/\n",
      "chat_logs.log\n",
      "chat_summary.ipynb\n",
      "convertion.txt\n",
      "\u001b[34mcrew\u001b[m\u001b[m/\n",
      "data_processor.py\n",
      "\u001b[34mdb\u001b[m\u001b[m/\n",
      "\u001b[34menv\u001b[m\u001b[m/\n",
      "job_description.yaml\n",
      "prompt_test.ipynb\n",
      "questions.yaml\n",
      "requirements.txt\n",
      "\u001b[34mrouters\u001b[m\u001b[m/\n",
      "stremletapp.py\n",
      "test.ipynb\n",
      "vector_search.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 01:11:48,526 - 8606380608 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "2024-10-21 01:11:48,530 - 8606380608 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "2024-10-21 01:11:48,534 - 8606380608 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "2024-10-21 01:11:48,561 - 8606380608 - __init__.py-__init__:538 - WARNING: Overriding of current TracerProvider is not allowed\n",
      "2024-10-21 01:11:48,563 - 8606380608 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import BaseTool, tool\n",
    "import os\n",
    "from textwrap import dedent\n",
    "\n",
    "\n",
    "question='What is your desired salary range?'\n",
    "usermessage=' i am instread in this job'\n",
    "job_description='''Job Title: Senior Software Engineer - Full Stack Developer\n",
    "Location: Bangalore, India\n",
    "\n",
    "Company: TechInnovate Solutions Pvt. Ltd.\n",
    "\n",
    "Job Description:\n",
    "TechInnovate Solutions, a leading software development company in Bangalore, is looking for a Senior Full Stack Developer to join our dynamic team. The candidate will be responsible for developing scalable web applications, contributing to architecture design, and mentoring junior developers.\n",
    "\n",
    "Qualifications:\n",
    "\n",
    "\t•\tBachelor’s or Master’s degree in Computer Science, Engineering, or a related field.\n",
    "\t•\t5+ years of full-stack development experience.\n",
    "\t•\tProven track record of leading software projects and mentoring teams.'''\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.groq.com/openai/v1\"\n",
    "os.environ[\"OPENAI_MODEL_NAME\"] = \"llama3-8b-8192\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\" \n",
    "\n",
    "\n",
    "class MyCustomTool(BaseTool):\n",
    "    name: str = \"QuestionAnswer\"\n",
    "    description: str = \"used to ave question and answer to database\"\n",
    "    def _run(self,question: str, answer:str) -> str:\n",
    "        if question and answer:\n",
    "            data={\"question\": question, \"answer\": answer}\n",
    "        return \"question and answer are saved to database now use chat tool to continue conversation\"\n",
    "\n",
    "class MyChatTool(BaseTool):\n",
    "    name: str = \"ChatTool\"\n",
    "    description: str = \"used to converse naturallly with candidate\"\n",
    "    def _run(self,usermessgae: str) -> str:\n",
    "        \n",
    "        return usermessgae\n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "# manager agent\n",
    "classifier = Agent(\n",
    "    role=\"Message Analyzer\",\n",
    "            backstory=dedent(\n",
    "                \"\"\" With expertise in natural language understanding, you can distinguish between casual conversations and specific answer to a questions. \n",
    "                    Your job is to direct the message to the correct agent based on its content.\"\"\"),\n",
    "            goal=dedent(\n",
    "                \"\"\"Analyze the user message to determine whether it should be handled by the Chat Agent or the Question-Answer Agent.\n",
    "                    - If the message is conversational or not an answer to the question, assign it to the Chat Agent.\n",
    "                    - If the message is a direct question, assign it to the Question-Answer Agent to be saved to a database.\n",
    "                    - if the Question-Answer Agent is called then call the normal chat Agent\n",
    "                    -the exact name of the agents are \n",
    "                        SimpleChat\n",
    "                        QuestionAnswer\"\"\"),\n",
    "            tools=[],\n",
    "            allow_delegation=False,\n",
    "            verbose=True,\n",
    ")\n",
    "\n",
    "# manager agent\n",
    "QA = Agent(\n",
    "    role=\"Question Answer Retriver\",\n",
    "            backstory=dedent(\n",
    "                \"\"\" With expertise in natural language understanding, you are able to retrive and generat a question answer pair.\"\"\"),\n",
    "            goal=dedent(\n",
    "                \"\"\"Analyze the incoming message and retrive Question and answer from it to later be saved to database.\n",
    "                To easily parse the output return the question and answer in the following format.\n",
    "\n",
    "                {'Question': [question]\n",
    "                'Answer': [answer]}\"\"\"),\n",
    "            tools=[],\n",
    "            allow_delegation=False,\n",
    "            verbose=True,\n",
    ")\n",
    "\n",
    "chatAgent=Agent(\n",
    "    role=\"Chat\",\n",
    "            backstory=dedent(\n",
    "                \"\"\"You are a friendly and professional recruiter helping clients find suitable jobs based on their qualifications, preferences, and career goals. \"\"\"),\n",
    "            goal=dedent(\n",
    "                \"\"\"\n",
    "                Your goal is to guide the client through a conversation to better understand their skills, job preferences, and experience. \n",
    "                Follow a predefined question flow but remain conversational and natural. Adjust your responses based on the client’s answers, offering helpful information and advice when needed.\n",
    "                You will also assist clients by answering questions about specific job descriptions, but you should only provide answers based strictly on the information in the job description provided. \n",
    "                If the answer is not found, continue the conversation without providing additional information. Do not reveal that you are an AI.\n",
    "                you will be provided with question which you have to naturally insert in the conversation to the candidate\n",
    "                Guidelines:\n",
    "\n",
    "                •\tMaintain a supportive and encouraging tone throughout the conversation.\n",
    "                •\tAdapt your responses based on the candidate’s answers.\n",
    "                •\tProvide relevant advice, such as explaining job trends, skills in demand, or locations with good opportunities.\n",
    "                •\tIf the information is not available in the context, say that it is not specified without making up any facts.\n",
    "                •\tDo not mention or imply that you are an AI or automated system.\n",
    "                •\tAfter gathering all necessary information, summarize their preferences and outline the next steps in the process.\"\"\"),\n",
    "            tools=[],\n",
    "            allow_delegation=False,\n",
    "            verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "classify_task = Task(\n",
    "    description=f\"\"\"Analyze if the given message is an answer to the given question or just a simple converstion\n",
    "    Question:\n",
    "    {question}\n",
    "    user message:\n",
    "    {usermessage}\n",
    "    \"\"\",\n",
    "    agent=classifier,\n",
    "    expected_output=\"{'agent':[agent name]}\"\n",
    ")\n",
    "\n",
    "QandATask=Task(\n",
    "    description=f\"\"\"Analyze if the given message and return a formated pair to be parsed and saved to database.\n",
    "    Question:\n",
    "    {question}\n",
    "    user message:\n",
    "    {usermessage}\n",
    "    \"\"\",\n",
    "    agent=QA,\n",
    "    expected_output=\"{'Question':[question], 'Answer':[answer]}\"\n",
    ")\n",
    "\n",
    "ChatTask=Task(\n",
    "    description=f\"\"\"\n",
    "    Question:\n",
    "    {question}\n",
    "    user message:\n",
    "    {usermessage}\n",
    "    job description:\n",
    "    {job_description}\n",
    "    \"\"\",\n",
    "    agent=chatAgent,\n",
    "    expected_output=\" responce to the user question or a simple converstion\"\n",
    ")\n",
    "# Create the Crew\n",
    "crew = Crew(\n",
    "    agents=[classifier],\n",
    "    tasks=[classify_task],\n",
    "    verbose=True,\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "# Execute the Crew\n",
    "# output = crew.kickoff()\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:02:41,740 - 8606380608 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mMessage Analyzer\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mAnalyze if the given message is an answer to the given question or just a simple converstion\n",
      "    Question:\n",
      "    What is your desired salary range?\n",
      "    user message:\n",
      "    I am expection a salary in the range of 8-10 lakhs\n",
      "    \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mMessage Analyzer\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "{'agent': 'QuestionAnswer'}\u001b[00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "\n",
    "def parse_llm_output(output):\n",
    "    try:\n",
    "        # Try parsing using json.loads (assumes valid JSON format with double quotes)\n",
    "        return json.loads(output)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            # If JSON parsing fails, try using ast.literal_eval for single quotes\n",
    "            return ast.literal_eval(output)\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            # Return a message if parsing fails\n",
    "            return f\"Parsing failed: {e}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question='What is your desired salary range?'\n",
    "usermessage='i am instread in this job'\n",
    "job_description='''Job Title: Senior Software Engineer - Full Stack Developer\n",
    "Location: Bangalore, India\n",
    "\n",
    "Company: TechInnovate Solutions Pvt. Ltd.\n",
    "\n",
    "Job Description:\n",
    "TechInnovate Solutions, a leading software development company in Bangalore, is looking for a Senior Full Stack Developer to join our dynamic team. The candidate will be responsible for developing scalable web applications, contributing to architecture design, and mentoring junior developers.\n",
    "\n",
    "Qualifications:\n",
    "\n",
    "\t•\tBachelor’s or Master’s degree in Computer Science, Engineering, or a related field.\n",
    "\t•\t5+ years of full-stack development experience.\n",
    "\t•\tProven track record of leading software projects and mentoring teams.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.groq.com/openai/v1\"\n",
    "os.environ[\"OPENAI_MODEL_NAME\"] = \"llama3-8b-8192\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 02:02:45,247 - 8606380608 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "2024-10-21 02:02:45,251 - 8606380608 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "2024-10-21 02:02:45,256 - 8606380608 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import BaseTool, tool\n",
    "import os\n",
    "from textwrap import dedent\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.groq.com/openai/v1\"\n",
    "os.environ[\"OPENAI_MODEL_NAME\"] = \"llama3-8b-8192\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\" \n",
    "\n",
    "classifier = Agent(\n",
    "    role=\"Message Analyzer\",\n",
    "            backstory=dedent(\n",
    "                \"\"\" With expertise in natural language understanding, you can distinguish between casual conversations and specific answer to a questions. \n",
    "                    Your job is to direct the message to the correct agent based on its content.\"\"\"),\n",
    "            goal=dedent(\n",
    "                \"\"\"Analyze the user message to determine whether it should be handled by the Chat Agent or the Question-Answer Agent.\n",
    "                    - If the message is conversational or not an answer to the question, assign it to the Chat Agent.\n",
    "                    - If the message is a direct question, assign it to the Question-Answer Agent to be saved to a database.\n",
    "                    - if the Question-Answer Agent is called then call the normal chat Agent\n",
    "                    -the exact name of the agents are \n",
    "                        SimpleChat\n",
    "                        QuestionAnswer\"\"\"),\n",
    "            tools=[],\n",
    "            allow_delegation=False,\n",
    "            verbose=True,\n",
    ")\n",
    "\n",
    "# manager agent\n",
    "QA = Agent(\n",
    "    role=\"Question Answer Retriver\",\n",
    "            backstory=dedent(\n",
    "                \"\"\" With expertise in natural language understanding, you are able to retrive and generat a question answer pair.\"\"\"),\n",
    "            goal=dedent(\n",
    "                \"\"\"Analyze the incoming message and retrive Question and answer from it to later be saved to database.\n",
    "                To easily parse the output return the question and answer in the following format.\n",
    "\n",
    "                {'Question': [question]\n",
    "                'Answer': [answer]}\"\"\"),\n",
    "            tools=[],\n",
    "            allow_delegation=False,\n",
    "            verbose=True,\n",
    ")\n",
    "\n",
    "chatAgent=Agent(\n",
    "    role=\"Chat\",\n",
    "            backstory=dedent(\n",
    "                \"\"\"You are a friendly and professional recruiter helping clients find suitable jobs based on their qualifications, preferences, and career goals. \"\"\"),\n",
    "            goal=dedent(\n",
    "                \"\"\"\n",
    "                Your goal is to guide the client through a conversation to better understand their skills, job preferences, and experience. \n",
    "                Follow a predefined question flow but remain conversational and natural. Adjust your responses based on the client’s answers, offering helpful information and advice when needed.\n",
    "                You will also assist clients by answering questions about specific job descriptions, but you should only provide answers based strictly on the information in the job description provided. \n",
    "                If the answer is not found, continue the conversation without providing additional information. Do not reveal that you are an AI.\n",
    "                you will be provided with question which you have to naturally insert in the conversation to the candidate\n",
    "                Guidelines:\n",
    "\n",
    "                •\tMaintain a supportive and encouraging tone throughout the conversation.\n",
    "                •\tAdapt your responses based on the candidate’s answers.\n",
    "                •\tProvide relevant advice, such as explaining job trends, skills in demand, or locations with good opportunities.\n",
    "                •\tIf the information is not available in the context, say that it is not specified without making up any facts.\n",
    "                •\tDo not mention or imply that you are an AI or automated system.\n",
    "                •\tAfter gathering all necessary information, summarize their preferences and outline the next steps in the process.\"\"\"),\n",
    "            tools=[],\n",
    "            allow_delegation=False,\n",
    "            verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "classify_task = Task(\n",
    "        description=f\"\"\"Analyze if the given message is an answer to the given question or just a simple converstion\n",
    "        Question:\n",
    "        {question}\n",
    "        user message:\n",
    "        {usermessage}\n",
    "        \"\"\",\n",
    "        agent=classifier,\n",
    "        expected_output=\"{'agent':[agent name]}\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "QandATask=Task(\n",
    "        description=f\"\"\"Analyze if the given message and return a formated pair to be parsed and saved to database.\n",
    "        Question:\n",
    "        {question}\n",
    "        user message:\n",
    "        {usermessage}\n",
    "        \"\"\",\n",
    "        agent=QA,\n",
    "        expected_output=\"{'Question':[question], 'Answer':[answer]}\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "ChatTask=Task(\n",
    "        description=f\"\"\"\n",
    "        Question:\n",
    "        {question}\n",
    "        user message:\n",
    "        {usermessage}\n",
    "        job description:\n",
    "        {job_description}\n",
    "        \"\"\",\n",
    "        agent=chatAgent,\n",
    "        expected_output=\" responce to the user question or a simple converstion\"\n",
    ")\n",
    "\n",
    "# crew = Crew(\n",
    "#     agents=[],\n",
    "#     tasks=[],\n",
    "#     verbose=True,\n",
    "#     process=Process.sequential\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 02:25:18,007 - 8606380608 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "2024-10-21 02:25:18,008 - 8606380608 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "2024-10-21 02:25:18,009 - 8606380608 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from app.agents import agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "\n",
    "def parse_llm_output(output):\n",
    "    try:\n",
    "        # Try parsing using json.loads (assumes valid JSON format with double quotes)\n",
    "        return json.loads(output)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            # If JSON parsing fails, try using ast.literal_eval for single quotes\n",
    "            return ast.literal_eval(output)\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            # Return a message if parsing fails\n",
    "            return f\"Parsing failed: {e}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 02:39:42,214 - 8606380608 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "2024-10-21 02:39:42,215 - 8606380608 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "2024-10-21 02:39:42,216 - 8606380608 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "2024-10-21 02:39:42,217 - 8606380608 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mMessage Analyzer\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mAnalyze if the given message is an answer to the given question or just a simple converstion\n",
      "        Question:\n",
      "        {question}\n",
      "        user message:\n",
      "        {usermessage}\n",
      "        \u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 02:39:44,056 - 8606380608 - llm.py-llm:178 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mMessage Analyzer\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "{'agent': 'SimpleChat'}\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mChat\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "        Question:\n",
      "        What is your desired salary range?\n",
      "        user message:\n",
      "        i am instread in this job\n",
      "        job description:\n",
      "        Job Title: Senior Software Engineer - Full Stack Developer\n",
      "Location: Bangalore, India\n",
      "\n",
      "Company: TechInnovate Solutions Pvt. Ltd.\n",
      "\n",
      "Job Description:\n",
      "TechInnovate Solutions, a leading software development company in Bangalore, is looking for a Senior Full Stack Developer to join our dynamic team. The candidate will be responsible for developing scalable web applications, contributing to architecture design, and mentoring junior developers.\n",
      "\n",
      "Qualifications:\n",
      "\n",
      "\t•\tBachelor’s or Master’s degree in Computer Science, Engineering, or a related field.\n",
      "\t•\t5+ years of full-stack development experience.\n",
      "\t•\tProven track record of leading software projects and mentoring teams.\n",
      "        \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mChat\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "I'm glad you're interested in the Senior Software Engineer - Full Stack Developer role at TechInnovate Solutions Pvt. Ltd. The job description mentions that the company is looking for a candidate with 5+ years of full-stack development experience, a proven track record of leading software projects, and mentoring teams. Can you tell me a bit more about your experience in full-stack development? Have you had any leadership or mentoring roles in your previous positions?\u001b[00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from app.agents import agents\n",
    "import os\n",
    "\n",
    "# classify_task=agents.ClassifyTask(question,usermessage)\n",
    "# QandATask=agents.QandATask(question,usermessage)\n",
    "# ChatTask=agents.ChatTask(question, usermessage, job_description)\n",
    "classify_task=agents.classify_task\n",
    "classify_task.description.format(question=question, usermessage=usermessage)\n",
    "\n",
    "output=agents.classifier.execute_task(classify_task)\n",
    "parsed_output = parse_llm_output(output)\n",
    "\n",
    "if parsed_output['agent']=='QuestionAnswer':\n",
    "    QandAtsak=agents.QandATask\n",
    "    QandAtsak.description.format(question=question,usermessage=usermessage)\n",
    "    QandAtsak.description\n",
    "    qaoutput=agents.classifier.execute_task(QandATask)\n",
    "    parse_llm_output(qaoutput)\n",
    "    ChatTask=agents.ChatTask\n",
    "    ChatTask.description=ChatTask.description.format(question=question,usermessage=usermessage,job_description=job_description)\n",
    "    newoutput=agents.chatAgent.execute_task(ChatTask)\n",
    "if parsed_output['agent']=='SimpleChat':\n",
    "    ChatTask=agents.ChatTask\n",
    "    ChatTask.description=ChatTask.description.format(question=question,usermessage=usermessage,job_description=job_description, context=context)\n",
    "    newoutput=agents.chatAgent.execute_task(ChatTask)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Analyze if the given message is an answer to the given question or just a simple converstion\\n        Question:\\n        What is your desired salary range?\\n        user message:\\n        i am instread in this job\\n        '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_task=agents.classify_task\n",
    "classify_task.description.format(question=question, usermessage=usermessage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task(description=\n",
       "        Question:\n",
       "        What is your desired salary range?\n",
       "        user message:\n",
       "        i am instread in this job\n",
       "        job description:\n",
       "        Job Title: Senior Software Engineer - Full Stack Developer\n",
       "Location: Bangalore, India\n",
       "\n",
       "Company: TechInnovate Solutions Pvt. Ltd.\n",
       "\n",
       "Job Description:\n",
       "TechInnovate Solutions, a leading software development company in Bangalore, is looking for a Senior Full Stack Developer to join our dynamic team. The candidate will be responsible for developing scalable web applications, contributing to architecture design, and mentoring junior developers.\n",
       "\n",
       "Qualifications:\n",
       "\n",
       "\t•\tBachelor’s or Master’s degree in Computer Science, Engineering, or a related field.\n",
       "\t•\t5+ years of full-stack development experience.\n",
       "\t•\tProven track record of leading software projects and mentoring teams.\n",
       "        , expected_output= responce to the user question or a simple converstion)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
